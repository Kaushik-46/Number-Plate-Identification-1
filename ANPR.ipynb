{"cells":[{"cell_type":"markdown","metadata":{"id":"QUANWN3rpfC9"},"source":["# 0. Setup Paths"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"146BB11JpfDA","executionInfo":{"status":"ok","timestamp":1672059599295,"user_tz":-330,"elapsed":466,"user":{"displayName":"Mani","userId":"17004703946425084322"}}},"outputs":[],"source":["import os"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"42hJEdo_pfDB","executionInfo":{"status":"ok","timestamp":1672059601528,"user_tz":-330,"elapsed":6,"user":{"displayName":"Mani","userId":"17004703946425084322"}}},"outputs":[],"source":["CUSTOM_MODEL_NAME = 'my_ssd_mobnet' \n","PRETRAINED_MODEL_NAME = 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8'\n","PRETRAINED_MODEL_URL = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz'\n","TF_RECORD_SCRIPT_NAME = 'generate_tfrecord.py'\n","LABEL_MAP_NAME = 'label_map.pbtxt'"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"hbPhYVy_pfDB","executionInfo":{"status":"ok","timestamp":1672059603364,"user_tz":-330,"elapsed":7,"user":{"displayName":"Mani","userId":"17004703946425084322"}}},"outputs":[],"source":["paths = {\n","    'WORKSPACE_PATH': os.path.join('Tensorflow', 'workspace'),\n","    'SCRIPTS_PATH': os.path.join('Tensorflow','scripts'),\n","    'APIMODEL_PATH': os.path.join('Tensorflow','models'),\n","    'ANNOTATION_PATH': os.path.join('Tensorflow', 'workspace','annotations'),\n","    'IMAGE_PATH': os.path.join('Tensorflow', 'workspace','images'),\n","    'MODEL_PATH': os.path.join('Tensorflow', 'workspace','models'),\n","    'PRETRAINED_MODEL_PATH': os.path.join('Tensorflow', 'workspace','pre-trained-models'),\n","    'CHECKPOINT_PATH': os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME), \n","    'OUTPUT_PATH': os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME, 'export'), \n","    'TFJS_PATH':os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME, 'tfjsexport'), \n","    'TFLITE_PATH':os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME, 'tfliteexport'), \n","    'PROTOC_PATH':os.path.join('Tensorflow','protoc')\n"," }"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"LwhWZMI0pfDC","executionInfo":{"status":"ok","timestamp":1672059603365,"user_tz":-330,"elapsed":7,"user":{"displayName":"Mani","userId":"17004703946425084322"}}},"outputs":[],"source":["files = {\n","    'PIPELINE_CONFIG':os.path.join('Tensorflow', 'workspace','models', CUSTOM_MODEL_NAME, 'pipeline.config'),\n","    'TF_RECORD_SCRIPT': os.path.join(paths['SCRIPTS_PATH'], TF_RECORD_SCRIPT_NAME), \n","    'LABELMAP': os.path.join(paths['ANNOTATION_PATH'], LABEL_MAP_NAME)\n","}"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"HR-TfDGrpfDC","executionInfo":{"status":"ok","timestamp":1672059604125,"user_tz":-330,"elapsed":13,"user":{"displayName":"Mani","userId":"17004703946425084322"}}},"outputs":[],"source":["for path in paths.values():\n","    if not os.path.exists(path):\n","        if os.name == 'posix':\n","            !mkdir -p {path}\n","        if os.name == 'nt':\n","            !mkdir {path}"]},{"cell_type":"markdown","metadata":{"id":"OLU-rs_ipfDE"},"source":["# 1. Download TF Models Pretrained Models from Tensorflow Model Zoo and Install TFOD"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"hX7bWga6-lz3","executionInfo":{"status":"ok","timestamp":1672059604748,"user_tz":-330,"elapsed":10,"user":{"displayName":"Mani","userId":"17004703946425084322"}}},"outputs":[],"source":["# https://www.tensorflow.org/install/source_windows"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"K-Cmz2edpfDE","scrolled":true,"executionInfo":{"status":"ok","timestamp":1672059604749,"user_tz":-330,"elapsed":9,"user":{"displayName":"Mani","userId":"17004703946425084322"}}},"outputs":[],"source":["if os.name=='nt':\n","    !pip install wget\n","    import wget"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"iA1DIq5OpfDE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672059632608,"user_tz":-330,"elapsed":27867,"user":{"displayName":"Mani","userId":"17004703946425084322"}},"outputId":"445aa0e1-60fb-443e-956b-48336465aa50"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'Tensorflow/models'...\n","remote: Enumerating objects: 80018, done.\u001b[K\n","remote: Counting objects: 100% (194/194), done.\u001b[K\n","remote: Compressing objects: 100% (119/119), done.\u001b[K\n","remote: Total 80018 (delta 87), reused 171 (delta 73), pack-reused 79824\u001b[K\n","Receiving objects: 100% (80018/80018), 594.45 MiB | 25.14 MiB/s, done.\n","Resolving deltas: 100% (56930/56930), done.\n"]}],"source":["if not os.path.exists(os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection')):\n","    !git clone https://github.com/tensorflow/models {paths['APIMODEL_PATH']}"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"rJjMHbnDs3Tv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672059858130,"user_tz":-330,"elapsed":87272,"user":{"displayName":"Mani","userId":"17004703946425084322"}},"outputId":"42a8b788-d2ad-4595-e3d4-15b1302fcba8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","protobuf-compiler is already the newest version (3.0.0-9.1ubuntu1).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","0 upgraded, 0 newly installed, 0 to remove and 20 not upgraded.\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Processing /content/Tensorflow/models/research\n","\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n","   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n","Collecting avro-python3\n","  Using cached avro-python3-1.10.2.tar.gz (38 kB)\n","Collecting apache-beam\n","  Using cached apache_beam-2.43.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.5 MB)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (7.1.2)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (4.9.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (3.2.0)\n","Requirement already satisfied: Cython in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (0.29.32)\n","Requirement already satisfied: contextlib2 in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (0.5.5)\n","Collecting tf-slim\n","  Using cached tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n","Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (1.15.0)\n","Requirement already satisfied: pycocotools in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (2.0.6)\n","Collecting lvis\n","  Using cached lvis-0.5.3-py3-none-any.whl (14 kB)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (1.7.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (1.3.5)\n","Collecting tf-models-official>=2.5.1\n","  Using cached tf_models_official-2.11.2-py2.py3-none-any.whl (2.3 MB)\n","Collecting tensorflow_io\n","  Using cached tensorflow_io-0.29.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (26.9 MB)\n","Requirement already satisfied: keras in /usr/local/lib/python3.8/dist-packages (from object-detection==0.1) (2.9.0)\n","Collecting pyparsing==2.4.7\n","  Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n","Collecting sacrebleu<=2.2.0\n","  Using cached sacrebleu-2.2.0-py3-none-any.whl (116 kB)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (1.21.6)\n","Collecting colorama\n","  Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Collecting portalocker\n","  Using cached portalocker-2.6.0-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (2022.6.2)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.8/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (0.8.10)\n","Collecting immutabledict\n","  Using cached immutabledict-2.2.3-py3-none-any.whl (4.0 kB)\n","Collecting tensorflow-addons\n","  Using cached tensorflow_addons-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.12.11)\n","Collecting seqeval\n","  Using cached seqeval-1.2.2.tar.gz (43 kB)\n","Collecting sentencepiece\n","  Using cached sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.6.0.66)\n","Requirement already satisfied: gin-config in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.5.0)\n","Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.6.0)\n","Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.12.0)\n","Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (5.4.8)\n","Requirement already satisfied: oauth2client in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.1.3)\n","Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.8/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.5.12)\n","Collecting tensorflow-model-optimization>=0.4.1\n","  Using cached tensorflow_model_optimization-0.7.3-py2.py3-none-any.whl (238 kB)\n","Collecting tensorflow~=2.11.0\n","  Using cached tensorflow-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n","Collecting tensorflow-text~=2.11.0\n","  Using cached tensorflow_text-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n","Collecting pyyaml<6.0,>=5.1\n","  Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB)\n","Collecting py-cpuinfo>=3.3.0\n","  Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n","Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.0.4)\n","Requirement already satisfied: google-auth<3dev,>=1.16.0 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.15.0)\n","Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.0.1)\n","Requirement already satisfied: google-api-core<3dev,>=1.21.0 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.8.2)\n","Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.8/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.17.4)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.57.0)\n","Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.8/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.23.0)\n","Requirement already satisfied: protobuf<5.0.0dev,>=3.15.0 in /usr/local/lib/python3.8/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.21.12)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (5.2.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.9)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.2.8)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.8/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (7.0.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2022.12.7)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2.8.2)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.24.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (4.64.1)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->object-detection==0.1) (2022.6)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.4.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.0.4)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (2.1.1)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (1.51.1)\n","Collecting tensorboard<2.12,>=2.11\n","  Downloading tensorboard-2.11.0-py3-none-any.whl (6.0 MB)\n","\u001b[K     |████████████████████████████████| 6.0 MB 14.9 MB/s \n","\u001b[?25hRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (0.2.0)\n","Collecting tensorflow-estimator<2.12,>=2.11.0\n","  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n","\u001b[K     |████████████████████████████████| 439 kB 78.6 MB/s \n","\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (3.1.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (4.4.0)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (0.28.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (14.0.6)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (1.3.0)\n","Collecting protobuf<5.0.0dev,>=3.15.0\n","  Downloading protobuf-3.19.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 58.0 MB/s \n","\u001b[?25hRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (1.6.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (57.4.0)\n","Collecting flatbuffers>=2.0\n","  Downloading flatbuffers-22.12.6-py2.py3-none-any.whl (26 kB)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (1.14.1)\n","Collecting keras\n","  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n","\u001b[K     |████████████████████████████████| 1.7 MB 50.3 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (21.3)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (0.38.4)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (3.4.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (1.8.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (0.6.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.6)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (1.0.1)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (5.1.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (3.11.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tf-models-official>=2.5.1->object-detection==0.1) (3.2.2)\n","Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official>=2.5.1->object-detection==0.1) (0.1.7)\n","Collecting fasteners<1.0,>=0.3\n","  Downloading fasteners-0.18-py3-none-any.whl (18 kB)\n","Collecting orjson<4.0\n","  Downloading orjson-3.8.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (278 kB)\n","\u001b[K     |████████████████████████████████| 278 kB 67.4 MB/s \n","\u001b[?25hCollecting objsize<0.6.0,>=0.5.2\n","  Downloading objsize-0.5.2-py3-none-any.whl (8.2 kB)\n","Collecting zstandard<1,>=0.18.0\n","  Downloading zstandard-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n","\u001b[K     |████████████████████████████████| 2.5 MB 68.0 MB/s \n","\u001b[?25hCollecting pymongo<4.0.0,>=3.8.0\n","  Downloading pymongo-3.13.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (526 kB)\n","\u001b[K     |████████████████████████████████| 526 kB 74.8 MB/s \n","\u001b[?25hRequirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (1.22.1)\n","Collecting dill<0.3.2,>=0.3.1.1\n","  Downloading dill-0.3.1.1.tar.gz (151 kB)\n","\u001b[K     |████████████████████████████████| 151 kB 80.1 MB/s \n","\u001b[?25hCollecting fastavro<2,>=0.23.6\n","  Downloading fastavro-1.7.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n","\u001b[K     |████████████████████████████████| 2.7 MB 61.6 MB/s \n","\u001b[?25hRequirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (1.3.0)\n","Collecting cloudpickle~=2.2.0\n","  Downloading cloudpickle-2.2.0-py3-none-any.whl (25 kB)\n","Collecting hdfs<3.0.0,>=2.1.0\n","  Downloading hdfs-2.7.0-py3-none-any.whl (34 kB)\n","Requirement already satisfied: pyarrow<10.0.0,>=0.15.1 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (9.0.0)\n","Collecting requests<3.0.0dev,>=2.18.0\n","  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n","\u001b[K     |████████████████████████████████| 62 kB 2.0 MB/s \n","\u001b[?25hRequirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.8/dist-packages (from apache-beam->object-detection==0.1) (1.7)\n","Collecting docopt\n","  Downloading docopt-0.6.2.tar.gz (25 kB)\n","Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.1.1)\n","Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.8/dist-packages (from lvis->object-detection==0.1) (4.6.0.66)\n","Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from lvis->object-detection==0.1) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from lvis->object-detection==0.1) (1.4.4)\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.8/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.3)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.8/dist-packages (from seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.0.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.2.0)\n","Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons->tf-models-official>=2.5.1->object-detection==0.1) (2.7.1)\n","Requirement already satisfied: promise in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (2.3)\n","Requirement already satisfied: toml in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (0.10.2)\n","Requirement already satisfied: etils[epath] in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (0.9.0)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (5.10.1)\n","Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (1.12.0)\n","Collecting tensorflow-io-gcs-filesystem>=0.23.1\n","  Downloading tensorflow_io_gcs_filesystem-0.29.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n","\u001b[K     |████████████████████████████████| 2.4 MB 53.6 MB/s \n","\u001b[?25hBuilding wheels for collected packages: object-detection, dill, avro-python3, docopt, seqeval\n","  Building wheel for object-detection (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for object-detection: filename=object_detection-0.1-py3-none-any.whl size=1696474 sha256=ee65383bc43ecc47bf7d2d67c3a7e260c668e0757edc9ea6ce3e3cb233c6ae92\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-16gh1xcc/wheels/5f/a2/bd/4f7680f297994e5417be0c4494525a7f286ebe68fc40f4ac38\n","  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78544 sha256=fb145bed99c670136b0394c2080f26412f42e0d5f9a1b5ea569fef37f013ae27\n","  Stored in directory: /root/.cache/pip/wheels/07/35/78/e9004fa30578734db7f10e7a211605f3f0778d2bdde38a239d\n","  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for avro-python3: filename=avro_python3-1.10.2-py3-none-any.whl size=44009 sha256=52ee75df4d46725516a2abfc610982b4729404bde0a60cf3a435d2733c8ff92f\n","  Stored in directory: /root/.cache/pip/wheels/bb/73/e9/d273421f5723c4bf544dcf9eb097bda94421ef8d3252699f0a\n","  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=7470c8fd96d9cadca995eeb20fa3642ce8ed8e77b154f6220309f66d2b8b4ca4\n","  Stored in directory: /root/.cache/pip/wheels/56/ea/58/ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16179 sha256=bfb5085fe26ea59cf6a690a922e7df5bb7aaf889381e12e4097967803e30906c\n","  Stored in directory: /root/.cache/pip/wheels/ad/5c/ba/05fa33fa5855777b7d686e843ec07452f22a66a138e290e732\n","Successfully built object-detection dill avro-python3 docopt seqeval\n","Installing collected packages: requests, pyparsing, protobuf, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, keras, flatbuffers, tensorflow, portalocker, docopt, dill, colorama, zstandard, tf-slim, tensorflow-text, tensorflow-model-optimization, tensorflow-addons, seqeval, sentencepiece, sacrebleu, pyyaml, pymongo, py-cpuinfo, orjson, objsize, immutabledict, hdfs, fasteners, fastavro, cloudpickle, tf-models-official, tensorflow-io, lvis, avro-python3, apache-beam, object-detection\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.23.0\n","    Uninstalling requests-2.23.0:\n","      Successfully uninstalled requests-2.23.0\n","  Attempting uninstall: pyparsing\n","    Found existing installation: pyparsing 3.0.9\n","    Uninstalling pyparsing-3.0.9:\n","      Successfully uninstalled pyparsing-3.0.9\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 4.21.12\n","    Uninstalling protobuf-4.21.12:\n","      Successfully uninstalled protobuf-4.21.12\n","  Attempting uninstall: tensorflow-io-gcs-filesystem\n","    Found existing installation: tensorflow-io-gcs-filesystem 0.28.0\n","    Uninstalling tensorflow-io-gcs-filesystem-0.28.0:\n","      Successfully uninstalled tensorflow-io-gcs-filesystem-0.28.0\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.9.0\n","    Uninstalling tensorflow-estimator-2.9.0:\n","      Successfully uninstalled tensorflow-estimator-2.9.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.9.1\n","    Uninstalling tensorboard-2.9.1:\n","      Successfully uninstalled tensorboard-2.9.1\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.9.0\n","    Uninstalling keras-2.9.0:\n","      Successfully uninstalled keras-2.9.0\n","  Attempting uninstall: flatbuffers\n","    Found existing installation: flatbuffers 1.12\n","    Uninstalling flatbuffers-1.12:\n","      Successfully uninstalled flatbuffers-1.12\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.9.2\n","    Uninstalling tensorflow-2.9.2:\n","      Successfully uninstalled tensorflow-2.9.2\n","  Attempting uninstall: dill\n","    Found existing installation: dill 0.3.6\n","    Uninstalling dill-0.3.6:\n","      Successfully uninstalled dill-0.3.6\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 6.0\n","    Uninstalling PyYAML-6.0:\n","      Successfully uninstalled PyYAML-6.0\n","  Attempting uninstall: pymongo\n","    Found existing installation: pymongo 4.3.3\n","    Uninstalling pymongo-4.3.3:\n","      Successfully uninstalled pymongo-4.3.3\n","  Attempting uninstall: cloudpickle\n","    Found existing installation: cloudpickle 1.5.0\n","    Uninstalling cloudpickle-1.5.0:\n","      Successfully uninstalled cloudpickle-1.5.0\n","Successfully installed apache-beam-2.43.0 avro-python3-1.10.2 cloudpickle-2.2.0 colorama-0.4.6 dill-0.3.1.1 docopt-0.6.2 fastavro-1.7.0 fasteners-0.18 flatbuffers-22.12.6 hdfs-2.7.0 immutabledict-2.2.3 keras-2.11.0 lvis-0.5.3 object-detection-0.1 objsize-0.5.2 orjson-3.8.3 portalocker-2.6.0 protobuf-3.19.6 py-cpuinfo-9.0.0 pymongo-3.13.0 pyparsing-2.4.7 pyyaml-5.4.1 requests-2.28.1 sacrebleu-2.2.0 sentencepiece-0.1.97 seqeval-1.2.2 tensorboard-2.11.0 tensorflow-2.11.0 tensorflow-addons-0.19.0 tensorflow-estimator-2.11.0 tensorflow-io-0.29.0 tensorflow-io-gcs-filesystem-0.29.0 tensorflow-model-optimization-0.7.3 tensorflow-text-2.11.0 tf-models-official-2.11.2 tf-slim-1.1.0 zstandard-0.19.0\n"]}],"source":["# Install Tensorflow Object Detection \n","if os.name=='posix':  \n","    !apt-get install protobuf-compiler\n","    !cd Tensorflow/models/research && protoc object_detection/protos/*.proto --python_out=. && cp object_detection/packages/tf2/setup.py . && python -m pip install . \n","    \n","if os.name=='nt':\n","    url=\"https://github.com/protocolbuffers/protobuf/releases/download/v3.15.6/protoc-3.15.6-win64.zip\"\n","    wget.download(url)\n","    !move protoc-3.15.6-win64.zip {paths['PROTOC_PATH']}\n","    !cd {paths['PROTOC_PATH']} && tar -xf protoc-3.15.6-win64.zip\n","    os.environ['PATH'] += os.pathsep + os.path.abspath(os.path.join(paths['PROTOC_PATH'], 'bin'))   \n","    !cd Tensorflow/models/research && protoc object_detection/protos/*.proto --python_out=. && copy object_detection\\\\packages\\\\tf2\\\\setup.py setup.py && python setup.py build && python setup.py install\n","    !cd Tensorflow/models/research/slim && pip install -e . "]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"I04lvQSq-lz-"},"outputs":[],"source":["!pip list"]},{"cell_type":"code","execution_count":21,"metadata":{"scrolled":true,"id":"pbdBDr1O-lz_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672059896214,"user_tz":-330,"elapsed":36233,"user":{"displayName":"Mani","userId":"17004703946425084322"}},"outputId":"5bddfc70-ec61-4c93-9be5-6cb0506dee00"},"outputs":[{"output_type":"stream","name":"stdout","text":["2022-12-26 13:04:21.725520: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2022-12-26 13:04:22.712199: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2022-12-26 13:04:22.712390: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2022-12-26 13:04:22.712413: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","Running tests under Python 3.8.16: /usr/bin/python3\n","[ RUN      ] ModelBuilderTF2Test.test_create_center_net_deepmac\n","2022-12-26 13:04:27.633201: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","W1226 13:04:27.904384 139654722787200 model_builder.py:1112] Building experimental DeepMAC meta-arch. Some features may be omitted.\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_deepmac): 1.4s\n","I1226 13:04:28.174005 139654722787200 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_center_net_deepmac): 1.4s\n","[       OK ] ModelBuilderTF2Test.test_create_center_net_deepmac\n","[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)): 0.66s\n","I1226 13:04:28.835819 139654722787200 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)): 0.66s\n","[       OK ] ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)\n","[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)): 0.28s\n","I1226 13:04:29.114214 139654722787200 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)): 0.28s\n","[       OK ] ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)\n","[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model_from_keypoints\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model_from_keypoints): 0.26s\n","I1226 13:04:29.379308 139654722787200 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_center_net_model_from_keypoints): 0.26s\n","[       OK ] ModelBuilderTF2Test.test_create_center_net_model_from_keypoints\n","[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model_mobilenet\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model_mobilenet): 1.87s\n","I1226 13:04:31.247970 139654722787200 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_center_net_model_mobilenet): 1.87s\n","[       OK ] ModelBuilderTF2Test.test_create_center_net_model_mobilenet\n","[ RUN      ] ModelBuilderTF2Test.test_create_experimental_model\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n","I1226 13:04:31.253450 139654722787200 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n","[       OK ] ModelBuilderTF2Test.test_create_experimental_model\n","[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.02s\n","I1226 13:04:31.277466 139654722787200 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.02s\n","[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n","[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.02s\n","I1226 13:04:31.293656 139654722787200 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.02s\n","[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n","[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.02s\n","I1226 13:04:31.310088 139654722787200 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.02s\n","[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n","[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.26s\n","I1226 13:04:31.568351 139654722787200 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.26s\n","[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n","[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.1s\n","I1226 13:04:31.664294 139654722787200 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.1s\n","[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n","[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.1s\n","I1226 13:04:31.767147 139654722787200 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.1s\n","[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n","[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.11s\n","I1226 13:04:31.877737 139654722787200 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.11s\n","[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n","[ RUN      ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.1s\n","I1226 13:04:31.974028 139654722787200 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.1s\n","[       OK ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n","[ RUN      ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.03s\n","I1226 13:04:32.002820 139654722787200 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.03s\n","[       OK ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n","[ RUN      ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n","I1226 13:04:32.184747 139654722787200 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b0\n","I1226 13:04:32.184900 139654722787200 ssd_efficientnet_bifpn_feature_extractor.py:152] EfficientDet BiFPN num filters: 64\n","I1226 13:04:32.184972 139654722787200 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 3\n","I1226 13:04:32.187112 139654722787200 efficientnet_model.py:143] round_filter input=32 output=32\n","I1226 13:04:32.212932 139654722787200 efficientnet_model.py:143] round_filter input=32 output=32\n","I1226 13:04:32.213063 139654722787200 efficientnet_model.py:143] round_filter input=16 output=16\n","I1226 13:04:32.293596 139654722787200 efficientnet_model.py:143] round_filter input=16 output=16\n","I1226 13:04:32.293769 139654722787200 efficientnet_model.py:143] round_filter input=24 output=24\n","I1226 13:04:32.481226 139654722787200 efficientnet_model.py:143] round_filter input=24 output=24\n","I1226 13:04:32.481398 139654722787200 efficientnet_model.py:143] round_filter input=40 output=40\n","I1226 13:04:32.661326 139654722787200 efficientnet_model.py:143] round_filter input=40 output=40\n","I1226 13:04:32.661484 139654722787200 efficientnet_model.py:143] round_filter input=80 output=80\n","I1226 13:04:32.954464 139654722787200 efficientnet_model.py:143] round_filter input=80 output=80\n","I1226 13:04:32.954657 139654722787200 efficientnet_model.py:143] round_filter input=112 output=112\n","I1226 13:04:33.231752 139654722787200 efficientnet_model.py:143] round_filter input=112 output=112\n","I1226 13:04:33.231918 139654722787200 efficientnet_model.py:143] round_filter input=192 output=192\n","I1226 13:04:33.581913 139654722787200 efficientnet_model.py:143] round_filter input=192 output=192\n","I1226 13:04:33.582080 139654722787200 efficientnet_model.py:143] round_filter input=320 output=320\n","I1226 13:04:33.668692 139654722787200 efficientnet_model.py:143] round_filter input=1280 output=1280\n","I1226 13:04:33.709014 139654722787200 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.0, resolution=224, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n","I1226 13:04:33.757388 139654722787200 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b1\n","I1226 13:04:33.757528 139654722787200 ssd_efficientnet_bifpn_feature_extractor.py:152] EfficientDet BiFPN num filters: 88\n","I1226 13:04:33.757595 139654722787200 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 4\n","I1226 13:04:33.759131 139654722787200 efficientnet_model.py:143] round_filter input=32 output=32\n","I1226 13:04:33.775118 139654722787200 efficientnet_model.py:143] round_filter input=32 output=32\n","I1226 13:04:33.775235 139654722787200 efficientnet_model.py:143] round_filter input=16 output=16\n","I1226 13:04:33.920899 139654722787200 efficientnet_model.py:143] round_filter input=16 output=16\n","I1226 13:04:33.921057 139654722787200 efficientnet_model.py:143] round_filter input=24 output=24\n","I1226 13:04:34.176566 139654722787200 efficientnet_model.py:143] round_filter input=24 output=24\n","I1226 13:04:34.176730 139654722787200 efficientnet_model.py:143] round_filter input=40 output=40\n","I1226 13:04:34.417917 139654722787200 efficientnet_model.py:143] round_filter input=40 output=40\n","I1226 13:04:34.418081 139654722787200 efficientnet_model.py:143] round_filter input=80 output=80\n","I1226 13:04:34.745371 139654722787200 efficientnet_model.py:143] round_filter input=80 output=80\n","I1226 13:04:34.745542 139654722787200 efficientnet_model.py:143] round_filter input=112 output=112\n","I1226 13:04:35.078028 139654722787200 efficientnet_model.py:143] round_filter input=112 output=112\n","I1226 13:04:35.078204 139654722787200 efficientnet_model.py:143] round_filter input=192 output=192\n","I1226 13:04:35.492244 139654722787200 efficientnet_model.py:143] round_filter input=192 output=192\n","I1226 13:04:35.492428 139654722787200 efficientnet_model.py:143] round_filter input=320 output=320\n","I1226 13:04:35.664334 139654722787200 efficientnet_model.py:143] round_filter input=1280 output=1280\n","I1226 13:04:35.695741 139654722787200 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.1, resolution=240, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n","I1226 13:04:35.757264 139654722787200 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b2\n","I1226 13:04:35.757830 139654722787200 ssd_efficientnet_bifpn_feature_extractor.py:152] EfficientDet BiFPN num filters: 112\n","I1226 13:04:35.757951 139654722787200 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 5\n","I1226 13:04:35.759814 139654722787200 efficientnet_model.py:143] round_filter input=32 output=32\n","I1226 13:04:35.776357 139654722787200 efficientnet_model.py:143] round_filter input=32 output=32\n","I1226 13:04:35.776463 139654722787200 efficientnet_model.py:143] round_filter input=16 output=16\n","I1226 13:04:35.904657 139654722787200 efficientnet_model.py:143] round_filter input=16 output=16\n","I1226 13:04:35.904819 139654722787200 efficientnet_model.py:143] round_filter input=24 output=24\n","I1226 13:04:36.156744 139654722787200 efficientnet_model.py:143] round_filter input=24 output=24\n","I1226 13:04:36.156908 139654722787200 efficientnet_model.py:143] round_filter input=40 output=48\n","I1226 13:04:36.580770 139654722787200 efficientnet_model.py:143] round_filter input=40 output=48\n","I1226 13:04:36.580931 139654722787200 efficientnet_model.py:143] round_filter input=80 output=88\n","I1226 13:04:36.930176 139654722787200 efficientnet_model.py:143] round_filter input=80 output=88\n","I1226 13:04:36.930349 139654722787200 efficientnet_model.py:143] round_filter input=112 output=120\n","I1226 13:04:37.279578 139654722787200 efficientnet_model.py:143] round_filter input=112 output=120\n","I1226 13:04:37.279753 139654722787200 efficientnet_model.py:143] round_filter input=192 output=208\n","I1226 13:04:37.702452 139654722787200 efficientnet_model.py:143] round_filter input=192 output=208\n","I1226 13:04:37.702625 139654722787200 efficientnet_model.py:143] round_filter input=320 output=352\n","I1226 13:04:37.877551 139654722787200 efficientnet_model.py:143] round_filter input=1280 output=1408\n","I1226 13:04:37.911185 139654722787200 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.1, depth_coefficient=1.2, resolution=260, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n","I1226 13:04:37.977055 139654722787200 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b3\n","I1226 13:04:37.977257 139654722787200 ssd_efficientnet_bifpn_feature_extractor.py:152] EfficientDet BiFPN num filters: 160\n","I1226 13:04:37.977355 139654722787200 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 6\n","I1226 13:04:37.979696 139654722787200 efficientnet_model.py:143] round_filter input=32 output=40\n","I1226 13:04:38.003460 139654722787200 efficientnet_model.py:143] round_filter input=32 output=40\n","I1226 13:04:38.003574 139654722787200 efficientnet_model.py:143] round_filter input=16 output=24\n","I1226 13:04:38.140999 139654722787200 efficientnet_model.py:143] round_filter input=16 output=24\n","I1226 13:04:38.141157 139654722787200 efficientnet_model.py:143] round_filter input=24 output=32\n","I1226 13:04:38.395939 139654722787200 efficientnet_model.py:143] round_filter input=24 output=32\n","I1226 13:04:38.396111 139654722787200 efficientnet_model.py:143] round_filter input=40 output=48\n","I1226 13:04:38.651129 139654722787200 efficientnet_model.py:143] round_filter input=40 output=48\n","I1226 13:04:38.651319 139654722787200 efficientnet_model.py:143] round_filter input=80 output=96\n","I1226 13:04:39.078448 139654722787200 efficientnet_model.py:143] round_filter input=80 output=96\n","I1226 13:04:39.078614 139654722787200 efficientnet_model.py:143] round_filter input=112 output=136\n","I1226 13:04:39.509045 139654722787200 efficientnet_model.py:143] round_filter input=112 output=136\n","I1226 13:04:39.509212 139654722787200 efficientnet_model.py:143] round_filter input=192 output=232\n","I1226 13:04:40.023826 139654722787200 efficientnet_model.py:143] round_filter input=192 output=232\n","I1226 13:04:40.024026 139654722787200 efficientnet_model.py:143] round_filter input=320 output=384\n","I1226 13:04:40.200551 139654722787200 efficientnet_model.py:143] round_filter input=1280 output=1536\n","I1226 13:04:40.237017 139654722787200 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.2, depth_coefficient=1.4, resolution=300, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n","I1226 13:04:40.300606 139654722787200 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b4\n","I1226 13:04:40.300745 139654722787200 ssd_efficientnet_bifpn_feature_extractor.py:152] EfficientDet BiFPN num filters: 224\n","I1226 13:04:40.300816 139654722787200 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 7\n","I1226 13:04:40.302371 139654722787200 efficientnet_model.py:143] round_filter input=32 output=48\n","I1226 13:04:40.320933 139654722787200 efficientnet_model.py:143] round_filter input=32 output=48\n","I1226 13:04:40.321050 139654722787200 efficientnet_model.py:143] round_filter input=16 output=24\n","I1226 13:04:40.465256 139654722787200 efficientnet_model.py:143] round_filter input=16 output=24\n","I1226 13:04:40.465415 139654722787200 efficientnet_model.py:143] round_filter input=24 output=32\n","I1226 13:04:40.792097 139654722787200 efficientnet_model.py:143] round_filter input=24 output=32\n","I1226 13:04:40.792279 139654722787200 efficientnet_model.py:143] round_filter input=40 output=56\n","I1226 13:04:41.148938 139654722787200 efficientnet_model.py:143] round_filter input=40 output=56\n","I1226 13:04:41.149113 139654722787200 efficientnet_model.py:143] round_filter input=80 output=112\n","I1226 13:04:41.672086 139654722787200 efficientnet_model.py:143] round_filter input=80 output=112\n","I1226 13:04:41.672271 139654722787200 efficientnet_model.py:143] round_filter input=112 output=160\n","I1226 13:04:42.208093 139654722787200 efficientnet_model.py:143] round_filter input=112 output=160\n","I1226 13:04:42.208269 139654722787200 efficientnet_model.py:143] round_filter input=192 output=272\n","I1226 13:04:43.114127 139654722787200 efficientnet_model.py:143] round_filter input=192 output=272\n","I1226 13:04:43.114317 139654722787200 efficientnet_model.py:143] round_filter input=320 output=448\n","I1226 13:04:43.286193 139654722787200 efficientnet_model.py:143] round_filter input=1280 output=1792\n","I1226 13:04:43.319636 139654722787200 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.4, depth_coefficient=1.8, resolution=380, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n","I1226 13:04:43.391539 139654722787200 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b5\n","I1226 13:04:43.391708 139654722787200 ssd_efficientnet_bifpn_feature_extractor.py:152] EfficientDet BiFPN num filters: 288\n","I1226 13:04:43.391786 139654722787200 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 7\n","I1226 13:04:43.394456 139654722787200 efficientnet_model.py:143] round_filter input=32 output=48\n","I1226 13:04:43.416178 139654722787200 efficientnet_model.py:143] round_filter input=32 output=48\n","I1226 13:04:43.416311 139654722787200 efficientnet_model.py:143] round_filter input=16 output=24\n","I1226 13:04:43.609766 139654722787200 efficientnet_model.py:143] round_filter input=16 output=24\n","I1226 13:04:43.609930 139654722787200 efficientnet_model.py:143] round_filter input=24 output=40\n","I1226 13:04:44.015885 139654722787200 efficientnet_model.py:143] round_filter input=24 output=40\n","I1226 13:04:44.016054 139654722787200 efficientnet_model.py:143] round_filter input=40 output=64\n","I1226 13:04:44.478701 139654722787200 efficientnet_model.py:143] round_filter input=40 output=64\n","I1226 13:04:44.478866 139654722787200 efficientnet_model.py:143] round_filter input=80 output=128\n","I1226 13:04:45.075067 139654722787200 efficientnet_model.py:143] round_filter input=80 output=128\n","I1226 13:04:45.075233 139654722787200 efficientnet_model.py:143] round_filter input=112 output=176\n","I1226 13:04:45.678755 139654722787200 efficientnet_model.py:143] round_filter input=112 output=176\n","I1226 13:04:45.678948 139654722787200 efficientnet_model.py:143] round_filter input=192 output=304\n","I1226 13:04:46.431535 139654722787200 efficientnet_model.py:143] round_filter input=192 output=304\n","I1226 13:04:46.431727 139654722787200 efficientnet_model.py:143] round_filter input=320 output=512\n","I1226 13:04:46.686673 139654722787200 efficientnet_model.py:143] round_filter input=1280 output=2048\n","I1226 13:04:46.723906 139654722787200 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.6, depth_coefficient=2.2, resolution=456, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n","I1226 13:04:46.805323 139654722787200 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b6\n","I1226 13:04:46.805470 139654722787200 ssd_efficientnet_bifpn_feature_extractor.py:152] EfficientDet BiFPN num filters: 384\n","I1226 13:04:46.805556 139654722787200 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 8\n","I1226 13:04:46.807209 139654722787200 efficientnet_model.py:143] round_filter input=32 output=56\n","I1226 13:04:46.825922 139654722787200 efficientnet_model.py:143] round_filter input=32 output=56\n","I1226 13:04:46.826065 139654722787200 efficientnet_model.py:143] round_filter input=16 output=32\n","I1226 13:04:47.037389 139654722787200 efficientnet_model.py:143] round_filter input=16 output=32\n","I1226 13:04:47.037546 139654722787200 efficientnet_model.py:143] round_filter input=24 output=40\n","I1226 13:04:47.558194 139654722787200 efficientnet_model.py:143] round_filter input=24 output=40\n","I1226 13:04:47.558372 139654722787200 efficientnet_model.py:143] round_filter input=40 output=72\n","I1226 13:04:48.062305 139654722787200 efficientnet_model.py:143] round_filter input=40 output=72\n","I1226 13:04:48.062464 139654722787200 efficientnet_model.py:143] round_filter input=80 output=144\n","I1226 13:04:48.745753 139654722787200 efficientnet_model.py:143] round_filter input=80 output=144\n","I1226 13:04:48.745927 139654722787200 efficientnet_model.py:143] round_filter input=112 output=200\n","I1226 13:04:49.666684 139654722787200 efficientnet_model.py:143] round_filter input=112 output=200\n","I1226 13:04:49.666851 139654722787200 efficientnet_model.py:143] round_filter input=192 output=344\n","I1226 13:04:50.606351 139654722787200 efficientnet_model.py:143] round_filter input=192 output=344\n","I1226 13:04:50.606513 139654722787200 efficientnet_model.py:143] round_filter input=320 output=576\n","I1226 13:04:50.870877 139654722787200 efficientnet_model.py:143] round_filter input=1280 output=2304\n","I1226 13:04:50.903080 139654722787200 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.8, depth_coefficient=2.6, resolution=528, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n","I1226 13:04:50.992791 139654722787200 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b7\n","I1226 13:04:50.992964 139654722787200 ssd_efficientnet_bifpn_feature_extractor.py:152] EfficientDet BiFPN num filters: 384\n","I1226 13:04:50.993037 139654722787200 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 8\n","I1226 13:04:50.994648 139654722787200 efficientnet_model.py:143] round_filter input=32 output=64\n","I1226 13:04:51.015927 139654722787200 efficientnet_model.py:143] round_filter input=32 output=64\n","I1226 13:04:51.016079 139654722787200 efficientnet_model.py:143] round_filter input=16 output=32\n","I1226 13:04:51.292283 139654722787200 efficientnet_model.py:143] round_filter input=16 output=32\n","I1226 13:04:51.292461 139654722787200 efficientnet_model.py:143] round_filter input=24 output=48\n","I1226 13:04:51.879497 139654722787200 efficientnet_model.py:143] round_filter input=24 output=48\n","I1226 13:04:51.879662 139654722787200 efficientnet_model.py:143] round_filter input=40 output=80\n","I1226 13:04:52.455330 139654722787200 efficientnet_model.py:143] round_filter input=40 output=80\n","I1226 13:04:52.455493 139654722787200 efficientnet_model.py:143] round_filter input=80 output=160\n","I1226 13:04:53.279754 139654722787200 efficientnet_model.py:143] round_filter input=80 output=160\n","I1226 13:04:53.279953 139654722787200 efficientnet_model.py:143] round_filter input=112 output=224\n","I1226 13:04:54.098616 139654722787200 efficientnet_model.py:143] round_filter input=112 output=224\n","I1226 13:04:54.098787 139654722787200 efficientnet_model.py:143] round_filter input=192 output=384\n","I1226 13:04:55.167295 139654722787200 efficientnet_model.py:143] round_filter input=192 output=384\n","I1226 13:04:55.167474 139654722787200 efficientnet_model.py:143] round_filter input=320 output=640\n","I1226 13:04:55.514330 139654722787200 efficientnet_model.py:143] round_filter input=1280 output=2560\n","I1226 13:04:55.548536 139654722787200 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=2.0, depth_coefficient=3.1, resolution=600, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 23.88s\n","I1226 13:04:55.887087 139654722787200 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 23.88s\n","[       OK ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n","[ RUN      ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.0s\n","I1226 13:04:55.911914 139654722787200 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.0s\n","[       OK ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n","[ RUN      ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n","I1226 13:04:55.913537 139654722787200 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n","[       OK ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n","[ RUN      ] ModelBuilderTF2Test.test_invalid_model_config_proto\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n","I1226 13:04:55.913998 139654722787200 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n","[       OK ] ModelBuilderTF2Test.test_invalid_model_config_proto\n","[ RUN      ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n","I1226 13:04:55.915368 139654722787200 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n","[       OK ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n","[ RUN      ] ModelBuilderTF2Test.test_session\n","[  SKIPPED ] ModelBuilderTF2Test.test_session\n","[ RUN      ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n","I1226 13:04:55.916627 139654722787200 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n","[       OK ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n","[ RUN      ] ModelBuilderTF2Test.test_unknown_meta_architecture\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n","I1226 13:04:55.917037 139654722787200 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n","[       OK ] ModelBuilderTF2Test.test_unknown_meta_architecture\n","[ RUN      ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n","INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n","I1226 13:04:55.917950 139654722787200 test_util.py:2457] time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n","[       OK ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n","----------------------------------------------------------------------\n","Ran 24 tests in 29.142s\n","\n","OK (skipped=1)\n"]}],"source":["VERIFICATION_SCRIPT = os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection', 'builders', 'model_builder_tf2_test.py')\n","# Verify Installation\n","!python {VERIFICATION_SCRIPT}"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"-CK0Vjpw-l0A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672059699424,"user_tz":-330,"elapsed":4786,"user":{"displayName":"Mani","userId":"17004703946425084322"}},"outputId":"b1e90213-0e35-4482-9fa5-dd0603d86841"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorflow==2.4.1\n","  Downloading tensorflow-2.4.1-cp38-cp38-manylinux2010_x86_64.whl (394.4 MB)\n","\u001b[K     |█████▍                          | 66.4 MB 2.5 MB/s eta 0:02:10\n","\u001b[31mERROR: Operation cancelled by user\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install tensorflow==2.4.1 tensorflow-gpu==2.4.1 --upgrade"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sAO23Cyx-l0B"},"outputs":[],"source":["!pip uninstall protobuf matplotlib -y\n","!pip install protobuf matplotlib==3.2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8JtDWpXx-l0C"},"outputs":[],"source":["!pip install Pillow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UAo24LZ9-l0C"},"outputs":[],"source":["!pip install pyyaml"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qy4pucgv-l0E"},"outputs":[],"source":["!pip list"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"qW6Z8Gyj-l0E","executionInfo":{"status":"ok","timestamp":1672059940153,"user_tz":-330,"elapsed":718,"user":{"displayName":"Mani","userId":"17004703946425084322"}}},"outputs":[],"source":["import object_detection"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"csofht2npfDE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672060046910,"user_tz":-330,"elapsed":1571,"user":{"displayName":"Mani","userId":"17004703946425084322"}},"outputId":"edfd1270-6892-4c6f-fc68-a003ae19a76b"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-12-26 13:07:27--  http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz\n","Resolving download.tensorflow.org (download.tensorflow.org)... 108.177.127.128, 2a00:1450:4013:c07::80\n","Connecting to download.tensorflow.org (download.tensorflow.org)|108.177.127.128|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 20515344 (20M) [application/x-tar]\n","Saving to: ‘ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz’\n","\n","ssd_mobilenet_v2_fp 100%[===================>]  19.56M   117MB/s    in 0.2s    \n","\n","2022-12-26 13:07:27 (117 MB/s) - ‘ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz’ saved [20515344/20515344]\n","\n","ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/\n","ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/\n","ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0.data-00000-of-00001\n","ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/checkpoint\n","ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0.index\n","ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/pipeline.config\n","ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/saved_model/\n","ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/saved_model/saved_model.pb\n","ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/saved_model/variables/\n","ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/saved_model/variables/variables.data-00000-of-00001\n","ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/saved_model/variables/variables.index\n"]}],"source":["if os.name =='posix':\n","    !wget {PRETRAINED_MODEL_URL}\n","    !mv {PRETRAINED_MODEL_NAME+'.tar.gz'} {paths['PRETRAINED_MODEL_PATH']}\n","    !cd {paths['PRETRAINED_MODEL_PATH']} && tar -zxvf {PRETRAINED_MODEL_NAME+'.tar.gz'}\n","if os.name == 'nt':\n","    wget.download(PRETRAINED_MODEL_URL)\n","    !move {PRETRAINED_MODEL_NAME+'.tar.gz'} {paths['PRETRAINED_MODEL_PATH']}\n","    !cd {paths['PRETRAINED_MODEL_PATH']} && tar -zxvf {PRETRAINED_MODEL_NAME+'.tar.gz'}"]},{"cell_type":"markdown","metadata":{"id":"M5KJTnkfpfDC"},"source":["# 2. Create Label Map"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"p1BVDWo7pfDC","executionInfo":{"status":"ok","timestamp":1672060089825,"user_tz":-330,"elapsed":7,"user":{"displayName":"Mani","userId":"17004703946425084322"}}},"outputs":[],"source":["labels = [{'name':'licence', 'id':1}]\n","\n","with open(files['LABELMAP'], 'w') as f:\n","    for label in labels:\n","        f.write('item { \\n')\n","        f.write('\\tname:\\'{}\\'\\n'.format(label['name']))\n","        f.write('\\tid:{}\\n'.format(label['id']))\n","        f.write('}\\n')"]},{"cell_type":"markdown","metadata":{"id":"C88zyVELpfDC"},"source":["# 3. Create TF records"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"kvf5WccwrFGq","executionInfo":{"status":"ok","timestamp":1672060094990,"user_tz":-330,"elapsed":646,"user":{"displayName":"Mani","userId":"17004703946425084322"}}},"outputs":[],"source":["# OPTIONAL IF RUNNING ON COLAB\n","ARCHIVE_FILES = os.path.join(paths['IMAGE_PATH'], 'archive.tar.gz')\n","if os.path.exists(ARCHIVE_FILES):\n","  !tar -zxvf {ARCHIVE_FILES}"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"KWpb_BVUpfDD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672060098491,"user_tz":-330,"elapsed":648,"user":{"displayName":"Mani","userId":"17004703946425084322"}},"outputId":"50d0e855-12bc-4c99-feb1-f7968ec0f253"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'Tensorflow/scripts'...\n","remote: Enumerating objects: 3, done.\u001b[K\n","remote: Counting objects: 100% (3/3), done.\u001b[K\n","remote: Compressing objects: 100% (2/2), done.\u001b[K\n","remote: Total 3 (delta 0), reused 1 (delta 0), pack-reused 0\u001b[K\n","Unpacking objects: 100% (3/3), done.\n"]}],"source":["if not os.path.exists(files['TF_RECORD_SCRIPT']):\n","    !git clone https://github.com/nicknochnack/GenerateTFRecord {paths['SCRIPTS_PATH']}"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"UPFToGZqpfDD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672060603266,"user_tz":-330,"elapsed":8436,"user":{"displayName":"Mani","userId":"17004703946425084322"}},"outputId":"93b09fbf-1958-48ac-f4f6-6e96aeda7e2a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Successfully created the TFRecord file: Tensorflow/workspace/annotations/train.record\n","Successfully created the TFRecord file: Tensorflow/workspace/annotations/test.record\n"]}],"source":["!python {files['TF_RECORD_SCRIPT']} -x {os.path.join(paths['IMAGE_PATH'], 'train')} -l {files['LABELMAP']} -o {os.path.join(paths['ANNOTATION_PATH'], 'train.record')} \n","!python {files['TF_RECORD_SCRIPT']} -x {os.path.join(paths['IMAGE_PATH'], 'test')} -l {files['LABELMAP']} -o {os.path.join(paths['ANNOTATION_PATH'], 'test.record')} "]},{"cell_type":"markdown","metadata":{"id":"qT4QU7pLpfDE"},"source":["# 4. Copy Model Config to Training Folder"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"cOjuTFbwpfDF","executionInfo":{"status":"ok","timestamp":1672060641711,"user_tz":-330,"elapsed":552,"user":{"displayName":"Mani","userId":"17004703946425084322"}}},"outputs":[],"source":["if os.name =='posix':\n","    !cp {os.path.join(paths['PRETRAINED_MODEL_PATH'], PRETRAINED_MODEL_NAME, 'pipeline.config')} {os.path.join(paths['CHECKPOINT_PATH'])}\n","if os.name == 'nt':\n","    !copy {os.path.join(paths['PRETRAINED_MODEL_PATH'], PRETRAINED_MODEL_NAME, 'pipeline.config')} {os.path.join(paths['CHECKPOINT_PATH'])}"]},{"cell_type":"markdown","metadata":{"id":"Ga8gpNslpfDF"},"source":["# 5. Update Config For Transfer Learning"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"Z9hRrO_ppfDF","executionInfo":{"status":"ok","timestamp":1672060647834,"user_tz":-330,"elapsed":2643,"user":{"displayName":"Mani","userId":"17004703946425084322"}}},"outputs":[],"source":["import tensorflow as tf\n","from object_detection.utils import config_util\n","from object_detection.protos import pipeline_pb2\n","from google.protobuf import text_format"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"c2A0mn4ipfDF","executionInfo":{"status":"ok","timestamp":1672060647835,"user_tz":-330,"elapsed":33,"user":{"displayName":"Mani","userId":"17004703946425084322"}}},"outputs":[],"source":["config = config_util.get_configs_from_pipeline_file(files['PIPELINE_CONFIG'])"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"uQA13-afpfDF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672060647836,"user_tz":-330,"elapsed":31,"user":{"displayName":"Mani","userId":"17004703946425084322"}},"outputId":"fd38e90e-3a59-4493-df35-07706b46fcac"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'model': ssd {\n","   num_classes: 90\n","   image_resizer {\n","     fixed_shape_resizer {\n","       height: 320\n","       width: 320\n","     }\n","   }\n","   feature_extractor {\n","     type: \"ssd_mobilenet_v2_fpn_keras\"\n","     depth_multiplier: 1.0\n","     min_depth: 16\n","     conv_hyperparams {\n","       regularizer {\n","         l2_regularizer {\n","           weight: 3.9999998989515007e-05\n","         }\n","       }\n","       initializer {\n","         random_normal_initializer {\n","           mean: 0.0\n","           stddev: 0.009999999776482582\n","         }\n","       }\n","       activation: RELU_6\n","       batch_norm {\n","         decay: 0.996999979019165\n","         scale: true\n","         epsilon: 0.0010000000474974513\n","       }\n","     }\n","     use_depthwise: true\n","     override_base_feature_extractor_hyperparams: true\n","     fpn {\n","       min_level: 3\n","       max_level: 7\n","       additional_layer_depth: 128\n","     }\n","   }\n","   box_coder {\n","     faster_rcnn_box_coder {\n","       y_scale: 10.0\n","       x_scale: 10.0\n","       height_scale: 5.0\n","       width_scale: 5.0\n","     }\n","   }\n","   matcher {\n","     argmax_matcher {\n","       matched_threshold: 0.5\n","       unmatched_threshold: 0.5\n","       ignore_thresholds: false\n","       negatives_lower_than_unmatched: true\n","       force_match_for_each_row: true\n","       use_matmul_gather: true\n","     }\n","   }\n","   similarity_calculator {\n","     iou_similarity {\n","     }\n","   }\n","   box_predictor {\n","     weight_shared_convolutional_box_predictor {\n","       conv_hyperparams {\n","         regularizer {\n","           l2_regularizer {\n","             weight: 3.9999998989515007e-05\n","           }\n","         }\n","         initializer {\n","           random_normal_initializer {\n","             mean: 0.0\n","             stddev: 0.009999999776482582\n","           }\n","         }\n","         activation: RELU_6\n","         batch_norm {\n","           decay: 0.996999979019165\n","           scale: true\n","           epsilon: 0.0010000000474974513\n","         }\n","       }\n","       depth: 128\n","       num_layers_before_predictor: 4\n","       kernel_size: 3\n","       class_prediction_bias_init: -4.599999904632568\n","       share_prediction_tower: true\n","       use_depthwise: true\n","     }\n","   }\n","   anchor_generator {\n","     multiscale_anchor_generator {\n","       min_level: 3\n","       max_level: 7\n","       anchor_scale: 4.0\n","       aspect_ratios: 1.0\n","       aspect_ratios: 2.0\n","       aspect_ratios: 0.5\n","       scales_per_octave: 2\n","     }\n","   }\n","   post_processing {\n","     batch_non_max_suppression {\n","       score_threshold: 9.99999993922529e-09\n","       iou_threshold: 0.6000000238418579\n","       max_detections_per_class: 100\n","       max_total_detections: 100\n","       use_static_shapes: false\n","     }\n","     score_converter: SIGMOID\n","   }\n","   normalize_loss_by_num_matches: true\n","   loss {\n","     localization_loss {\n","       weighted_smooth_l1 {\n","       }\n","     }\n","     classification_loss {\n","       weighted_sigmoid_focal {\n","         gamma: 2.0\n","         alpha: 0.25\n","       }\n","     }\n","     classification_weight: 1.0\n","     localization_weight: 1.0\n","   }\n","   encode_background_as_zeros: true\n","   normalize_loc_loss_by_codesize: true\n","   inplace_batchnorm_update: true\n","   freeze_batchnorm: false\n"," }, 'train_config': batch_size: 128\n"," data_augmentation_options {\n","   random_horizontal_flip {\n","   }\n"," }\n"," data_augmentation_options {\n","   random_crop_image {\n","     min_object_covered: 0.0\n","     min_aspect_ratio: 0.75\n","     max_aspect_ratio: 3.0\n","     min_area: 0.75\n","     max_area: 1.0\n","     overlap_thresh: 0.0\n","   }\n"," }\n"," sync_replicas: true\n"," optimizer {\n","   momentum_optimizer {\n","     learning_rate {\n","       cosine_decay_learning_rate {\n","         learning_rate_base: 0.07999999821186066\n","         total_steps: 50000\n","         warmup_learning_rate: 0.026666000485420227\n","         warmup_steps: 1000\n","       }\n","     }\n","     momentum_optimizer_value: 0.8999999761581421\n","   }\n","   use_moving_average: false\n"," }\n"," fine_tune_checkpoint: \"PATH_TO_BE_CONFIGURED\"\n"," num_steps: 50000\n"," startup_delay_steps: 0.0\n"," replicas_to_aggregate: 8\n"," max_number_of_boxes: 100\n"," unpad_groundtruth_tensors: false\n"," fine_tune_checkpoint_type: \"classification\"\n"," fine_tune_checkpoint_version: V2, 'train_input_config': label_map_path: \"PATH_TO_BE_CONFIGURED\"\n"," tf_record_input_reader {\n","   input_path: \"PATH_TO_BE_CONFIGURED\"\n"," }, 'eval_config': metrics_set: \"coco_detection_metrics\"\n"," use_moving_averages: false, 'eval_input_configs': [label_map_path: \"PATH_TO_BE_CONFIGURED\"\n"," shuffle: false\n"," num_epochs: 1\n"," tf_record_input_reader {\n","   input_path: \"PATH_TO_BE_CONFIGURED\"\n"," }\n"," ], 'eval_input_config': label_map_path: \"PATH_TO_BE_CONFIGURED\"\n"," shuffle: false\n"," num_epochs: 1\n"," tf_record_input_reader {\n","   input_path: \"PATH_TO_BE_CONFIGURED\"\n"," }}"]},"metadata":{},"execution_count":31}],"source":["config"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"9vK5lotDpfDF","executionInfo":{"status":"ok","timestamp":1672060653916,"user_tz":-330,"elapsed":719,"user":{"displayName":"Mani","userId":"17004703946425084322"}}},"outputs":[],"source":["pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n","with tf.io.gfile.GFile(files['PIPELINE_CONFIG'], \"r\") as f:                                                                                                                                                                                                                     \n","    proto_str = f.read()                                                                                                                                                                                                                                          \n","    text_format.Merge(proto_str, pipeline_config)  "]},{"cell_type":"code","execution_count":33,"metadata":{"id":"rP43Ph0JpfDG","executionInfo":{"status":"ok","timestamp":1672060654660,"user_tz":-330,"elapsed":6,"user":{"displayName":"Mani","userId":"17004703946425084322"}}},"outputs":[],"source":["pipeline_config.model.ssd.num_classes = len(labels)\n","pipeline_config.train_config.batch_size = 4\n","pipeline_config.train_config.fine_tune_checkpoint = os.path.join(paths['PRETRAINED_MODEL_PATH'], PRETRAINED_MODEL_NAME, 'checkpoint', 'ckpt-0')\n","pipeline_config.train_config.fine_tune_checkpoint_type = \"detection\"\n","pipeline_config.train_input_reader.label_map_path= files['LABELMAP']\n","pipeline_config.train_input_reader.tf_record_input_reader.input_path[:] = [os.path.join(paths['ANNOTATION_PATH'], 'train.record')]\n","pipeline_config.eval_input_reader[0].label_map_path = files['LABELMAP']\n","pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[:] = [os.path.join(paths['ANNOTATION_PATH'], 'test.record')]"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"oJvfgwWqpfDG","executionInfo":{"status":"ok","timestamp":1672060655578,"user_tz":-330,"elapsed":3,"user":{"displayName":"Mani","userId":"17004703946425084322"}}},"outputs":[],"source":["config_text = text_format.MessageToString(pipeline_config)                                                                                                                                                                                                        \n","with tf.io.gfile.GFile(files['PIPELINE_CONFIG'], \"wb\") as f:                                                                                                                                                                                                                     \n","    f.write(config_text)   "]},{"cell_type":"markdown","metadata":{"id":"Zr3ON7xMpfDG"},"source":["# 6. Train the model"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"B-Y2UQmQpfDG","executionInfo":{"status":"ok","timestamp":1672060657435,"user_tz":-330,"elapsed":3,"user":{"displayName":"Mani","userId":"17004703946425084322"}}},"outputs":[],"source":["TRAINING_SCRIPT = os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection', 'model_main_tf2.py')"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"jMP2XDfQpfDH","executionInfo":{"status":"ok","timestamp":1672060658094,"user_tz":-330,"elapsed":3,"user":{"displayName":"Mani","userId":"17004703946425084322"}}},"outputs":[],"source":["command = \"python {} --model_dir={} --pipeline_config_path={} --num_train_steps=10000\".format(TRAINING_SCRIPT, paths['CHECKPOINT_PATH'],files['PIPELINE_CONFIG'])"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"A4OXXi-ApfDH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672060658668,"user_tz":-330,"elapsed":8,"user":{"displayName":"Mani","userId":"17004703946425084322"}},"outputId":"95c07959-562e-45f4-bf06-70af3562e5de"},"outputs":[{"output_type":"stream","name":"stdout","text":["python Tensorflow/models/research/object_detection/model_main_tf2.py --model_dir=Tensorflow/workspace/models/my_ssd_mobnet --pipeline_config_path=Tensorflow/workspace/models/my_ssd_mobnet/pipeline.config --num_train_steps=10000\n"]}],"source":["print(command)"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"i3ZsJR-qpfDH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672061506313,"user_tz":-330,"elapsed":847048,"user":{"displayName":"Mani","userId":"17004703946425084322"}},"outputId":"de4eb5b2-b2cd-4dcb-b157-9e5abafe181e"},"outputs":[{"output_type":"stream","name":"stdout","text":["2022-12-26 13:17:41.873516: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2022-12-26 13:17:41.873631: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2022-12-26 13:17:41.873651: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","2022-12-26 13:17:46.029077: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n","I1226 13:17:46.044486 140232156051328 mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n","INFO:tensorflow:Maybe overwriting train_steps: 10000\n","I1226 13:17:46.048330 140232156051328 config_util.py:552] Maybe overwriting train_steps: 10000\n","INFO:tensorflow:Maybe overwriting use_bfloat16: False\n","I1226 13:17:46.048493 140232156051328 config_util.py:552] Maybe overwriting use_bfloat16: False\n","WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/object_detection/model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","rename to distribute_datasets_from_function\n","W1226 13:17:46.072609 140232156051328 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/object_detection/model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","rename to distribute_datasets_from_function\n","INFO:tensorflow:Reading unweighted datasets: ['Tensorflow/workspace/annotations/train.record']\n","I1226 13:17:46.079575 140232156051328 dataset_builder.py:162] Reading unweighted datasets: ['Tensorflow/workspace/annotations/train.record']\n","INFO:tensorflow:Reading record datasets for input file: ['Tensorflow/workspace/annotations/train.record']\n","I1226 13:17:46.079766 140232156051328 dataset_builder.py:79] Reading record datasets for input file: ['Tensorflow/workspace/annotations/train.record']\n","INFO:tensorflow:Number of filenames to read: 1\n","I1226 13:17:46.079848 140232156051328 dataset_builder.py:80] Number of filenames to read: 1\n","WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n","W1226 13:17:46.079914 140232156051328 dataset_builder.py:86] num_readers has been reduced to 1 to match input file shards.\n","WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n","W1226 13:17:46.085347 140232156051328 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n","WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.map()\n","W1226 13:17:46.101847 140232156051328 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.map()\n","WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n","Instructions for updating:\n","Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n","W1226 13:17:46.651150 140232156051328 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n","Instructions for updating:\n","Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n","WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n","W1226 13:17:52.100757 140232156051328 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n","W1226 13:17:54.610705 140232156051328 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","W1226 13:17:56.025817 140232156051328 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","/usr/local/lib/python3.8/dist-packages/keras/backend.py:451: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n","  warnings.warn(\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I1226 13:18:23.750792 140232156051328 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I1226 13:18:23.753410 140232156051328 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I1226 13:18:23.754459 140232156051328 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I1226 13:18:23.755397 140232156051328 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I1226 13:18:23.758587 140232156051328 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I1226 13:18:23.759511 140232156051328 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I1226 13:18:23.760539 140232156051328 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I1226 13:18:23.761477 140232156051328 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I1226 13:18:23.765399 140232156051328 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","I1226 13:18:23.766323 140232156051328 cross_device_ops.py:616] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use fn_output_signature instead\n","W1226 13:18:25.060895 140227704243968 deprecation.py:554] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use fn_output_signature instead\n","INFO:tensorflow:Step 100 per-step time 0.396s\n","I1226 13:19:04.231209 140232156051328 model_lib_v2.py:705] Step 100 per-step time 0.396s\n","INFO:tensorflow:{'Loss/classification_loss': 0.30316222,\n"," 'Loss/localization_loss': 0.45497048,\n"," 'Loss/regularization_loss': 0.15426345,\n"," 'Loss/total_loss': 0.91239613,\n"," 'learning_rate': 0.0319994}\n","I1226 13:19:04.231528 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.30316222,\n"," 'Loss/localization_loss': 0.45497048,\n"," 'Loss/regularization_loss': 0.15426345,\n"," 'Loss/total_loss': 0.91239613,\n"," 'learning_rate': 0.0319994}\n","INFO:tensorflow:Step 200 per-step time 0.070s\n","I1226 13:19:11.249904 140232156051328 model_lib_v2.py:705] Step 200 per-step time 0.070s\n","INFO:tensorflow:{'Loss/classification_loss': 0.26439387,\n"," 'Loss/localization_loss': 0.36510745,\n"," 'Loss/regularization_loss': 0.15445578,\n"," 'Loss/total_loss': 0.7839571,\n"," 'learning_rate': 0.0373328}\n","I1226 13:19:11.250286 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.26439387,\n"," 'Loss/localization_loss': 0.36510745,\n"," 'Loss/regularization_loss': 0.15445578,\n"," 'Loss/total_loss': 0.7839571,\n"," 'learning_rate': 0.0373328}\n","INFO:tensorflow:Step 300 per-step time 0.072s\n","I1226 13:19:18.429485 140232156051328 model_lib_v2.py:705] Step 300 per-step time 0.072s\n","INFO:tensorflow:{'Loss/classification_loss': 0.20088601,\n"," 'Loss/localization_loss': 0.22059835,\n"," 'Loss/regularization_loss': 0.15467267,\n"," 'Loss/total_loss': 0.57615703,\n"," 'learning_rate': 0.0426662}\n","I1226 13:19:18.429772 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.20088601,\n"," 'Loss/localization_loss': 0.22059835,\n"," 'Loss/regularization_loss': 0.15467267,\n"," 'Loss/total_loss': 0.57615703,\n"," 'learning_rate': 0.0426662}\n","INFO:tensorflow:Step 400 per-step time 0.100s\n","I1226 13:19:28.407108 140232156051328 model_lib_v2.py:705] Step 400 per-step time 0.100s\n","INFO:tensorflow:{'Loss/classification_loss': 0.22137943,\n"," 'Loss/localization_loss': 0.33653715,\n"," 'Loss/regularization_loss': 0.15484203,\n"," 'Loss/total_loss': 0.7127586,\n"," 'learning_rate': 0.047999598}\n","I1226 13:19:28.407488 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.22137943,\n"," 'Loss/localization_loss': 0.33653715,\n"," 'Loss/regularization_loss': 0.15484203,\n"," 'Loss/total_loss': 0.7127586,\n"," 'learning_rate': 0.047999598}\n","INFO:tensorflow:Step 500 per-step time 0.091s\n","I1226 13:19:37.534530 140232156051328 model_lib_v2.py:705] Step 500 per-step time 0.091s\n","INFO:tensorflow:{'Loss/classification_loss': 0.24123508,\n"," 'Loss/localization_loss': 0.23185207,\n"," 'Loss/regularization_loss': 0.15509923,\n"," 'Loss/total_loss': 0.62818635,\n"," 'learning_rate': 0.053333}\n","I1226 13:19:37.534820 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.24123508,\n"," 'Loss/localization_loss': 0.23185207,\n"," 'Loss/regularization_loss': 0.15509923,\n"," 'Loss/total_loss': 0.62818635,\n"," 'learning_rate': 0.053333}\n","INFO:tensorflow:Step 600 per-step time 0.072s\n","I1226 13:19:44.714873 140232156051328 model_lib_v2.py:705] Step 600 per-step time 0.072s\n","INFO:tensorflow:{'Loss/classification_loss': 0.2519765,\n"," 'Loss/localization_loss': 0.26012164,\n"," 'Loss/regularization_loss': 0.15521625,\n"," 'Loss/total_loss': 0.6673144,\n"," 'learning_rate': 0.0586664}\n","I1226 13:19:44.715196 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.2519765,\n"," 'Loss/localization_loss': 0.26012164,\n"," 'Loss/regularization_loss': 0.15521625,\n"," 'Loss/total_loss': 0.6673144,\n"," 'learning_rate': 0.0586664}\n","INFO:tensorflow:Step 700 per-step time 0.075s\n","I1226 13:19:52.192831 140232156051328 model_lib_v2.py:705] Step 700 per-step time 0.075s\n","INFO:tensorflow:{'Loss/classification_loss': 0.27879617,\n"," 'Loss/localization_loss': 0.240575,\n"," 'Loss/regularization_loss': 0.1553952,\n"," 'Loss/total_loss': 0.67476636,\n"," 'learning_rate': 0.0639998}\n","I1226 13:19:52.193116 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.27879617,\n"," 'Loss/localization_loss': 0.240575,\n"," 'Loss/regularization_loss': 0.1553952,\n"," 'Loss/total_loss': 0.67476636,\n"," 'learning_rate': 0.0639998}\n","INFO:tensorflow:Step 800 per-step time 0.073s\n","I1226 13:19:59.455483 140232156051328 model_lib_v2.py:705] Step 800 per-step time 0.073s\n","INFO:tensorflow:{'Loss/classification_loss': 0.32600442,\n"," 'Loss/localization_loss': 0.22052065,\n"," 'Loss/regularization_loss': 0.15557739,\n"," 'Loss/total_loss': 0.7021024,\n"," 'learning_rate': 0.069333196}\n","I1226 13:19:59.455772 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.32600442,\n"," 'Loss/localization_loss': 0.22052065,\n"," 'Loss/regularization_loss': 0.15557739,\n"," 'Loss/total_loss': 0.7021024,\n"," 'learning_rate': 0.069333196}\n","INFO:tensorflow:Step 900 per-step time 0.075s\n","I1226 13:20:06.907510 140232156051328 model_lib_v2.py:705] Step 900 per-step time 0.075s\n","INFO:tensorflow:{'Loss/classification_loss': 0.3768509,\n"," 'Loss/localization_loss': 0.43431923,\n"," 'Loss/regularization_loss': 0.15617554,\n"," 'Loss/total_loss': 0.96734565,\n"," 'learning_rate': 0.074666604}\n","I1226 13:20:06.907814 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.3768509,\n"," 'Loss/localization_loss': 0.43431923,\n"," 'Loss/regularization_loss': 0.15617554,\n"," 'Loss/total_loss': 0.96734565,\n"," 'learning_rate': 0.074666604}\n","INFO:tensorflow:Step 1000 per-step time 0.073s\n","I1226 13:20:14.255316 140232156051328 model_lib_v2.py:705] Step 1000 per-step time 0.073s\n","INFO:tensorflow:{'Loss/classification_loss': 0.29636344,\n"," 'Loss/localization_loss': 0.21104905,\n"," 'Loss/regularization_loss': 0.1562373,\n"," 'Loss/total_loss': 0.6636498,\n"," 'learning_rate': 0.08}\n","I1226 13:20:14.255614 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.29636344,\n"," 'Loss/localization_loss': 0.21104905,\n"," 'Loss/regularization_loss': 0.1562373,\n"," 'Loss/total_loss': 0.6636498,\n"," 'learning_rate': 0.08}\n","INFO:tensorflow:Step 1100 per-step time 0.084s\n","I1226 13:20:22.694005 140232156051328 model_lib_v2.py:705] Step 1100 per-step time 0.084s\n","INFO:tensorflow:{'Loss/classification_loss': 0.20799781,\n"," 'Loss/localization_loss': 0.15896307,\n"," 'Loss/regularization_loss': 0.15609017,\n"," 'Loss/total_loss': 0.523051,\n"," 'learning_rate': 0.07999918}\n","I1226 13:20:22.694316 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.20799781,\n"," 'Loss/localization_loss': 0.15896307,\n"," 'Loss/regularization_loss': 0.15609017,\n"," 'Loss/total_loss': 0.523051,\n"," 'learning_rate': 0.07999918}\n","INFO:tensorflow:Step 1200 per-step time 0.073s\n","I1226 13:20:29.933188 140232156051328 model_lib_v2.py:705] Step 1200 per-step time 0.073s\n","INFO:tensorflow:{'Loss/classification_loss': 0.1957959,\n"," 'Loss/localization_loss': 0.16083291,\n"," 'Loss/regularization_loss': 0.15610188,\n"," 'Loss/total_loss': 0.5127307,\n"," 'learning_rate': 0.079996705}\n","I1226 13:20:29.933490 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.1957959,\n"," 'Loss/localization_loss': 0.16083291,\n"," 'Loss/regularization_loss': 0.15610188,\n"," 'Loss/total_loss': 0.5127307,\n"," 'learning_rate': 0.079996705}\n","INFO:tensorflow:Step 1300 per-step time 0.072s\n","I1226 13:20:37.179526 140232156051328 model_lib_v2.py:705] Step 1300 per-step time 0.072s\n","INFO:tensorflow:{'Loss/classification_loss': 0.17505318,\n"," 'Loss/localization_loss': 0.16792697,\n"," 'Loss/regularization_loss': 0.15582615,\n"," 'Loss/total_loss': 0.4988063,\n"," 'learning_rate': 0.0799926}\n","I1226 13:20:37.179854 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.17505318,\n"," 'Loss/localization_loss': 0.16792697,\n"," 'Loss/regularization_loss': 0.15582615,\n"," 'Loss/total_loss': 0.4988063,\n"," 'learning_rate': 0.0799926}\n","INFO:tensorflow:Step 1400 per-step time 0.072s\n","I1226 13:20:44.397258 140232156051328 model_lib_v2.py:705] Step 1400 per-step time 0.072s\n","INFO:tensorflow:{'Loss/classification_loss': 0.2549778,\n"," 'Loss/localization_loss': 0.17059398,\n"," 'Loss/regularization_loss': 0.15584639,\n"," 'Loss/total_loss': 0.58141816,\n"," 'learning_rate': 0.07998685}\n","I1226 13:20:44.397543 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.2549778,\n"," 'Loss/localization_loss': 0.17059398,\n"," 'Loss/regularization_loss': 0.15584639,\n"," 'Loss/total_loss': 0.58141816,\n"," 'learning_rate': 0.07998685}\n","INFO:tensorflow:Step 1500 per-step time 0.072s\n","I1226 13:20:51.594314 140232156051328 model_lib_v2.py:705] Step 1500 per-step time 0.072s\n","INFO:tensorflow:{'Loss/classification_loss': 0.29590693,\n"," 'Loss/localization_loss': 0.21293569,\n"," 'Loss/regularization_loss': 0.15570416,\n"," 'Loss/total_loss': 0.6645467,\n"," 'learning_rate': 0.07997945}\n","I1226 13:20:51.594608 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.29590693,\n"," 'Loss/localization_loss': 0.21293569,\n"," 'Loss/regularization_loss': 0.15570416,\n"," 'Loss/total_loss': 0.6645467,\n"," 'learning_rate': 0.07997945}\n","INFO:tensorflow:Step 1600 per-step time 0.073s\n","I1226 13:20:58.862032 140232156051328 model_lib_v2.py:705] Step 1600 per-step time 0.073s\n","INFO:tensorflow:{'Loss/classification_loss': 0.18035154,\n"," 'Loss/localization_loss': 0.2717237,\n"," 'Loss/regularization_loss': 0.15539373,\n"," 'Loss/total_loss': 0.60746896,\n"," 'learning_rate': 0.079970405}\n","I1226 13:20:58.862343 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.18035154,\n"," 'Loss/localization_loss': 0.2717237,\n"," 'Loss/regularization_loss': 0.15539373,\n"," 'Loss/total_loss': 0.60746896,\n"," 'learning_rate': 0.079970405}\n","INFO:tensorflow:Step 1700 per-step time 0.071s\n","I1226 13:21:05.928673 140232156051328 model_lib_v2.py:705] Step 1700 per-step time 0.071s\n","INFO:tensorflow:{'Loss/classification_loss': 0.178843,\n"," 'Loss/localization_loss': 0.12257458,\n"," 'Loss/regularization_loss': 0.15541758,\n"," 'Loss/total_loss': 0.45683515,\n"," 'learning_rate': 0.07995972}\n","I1226 13:21:05.929069 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.178843,\n"," 'Loss/localization_loss': 0.12257458,\n"," 'Loss/regularization_loss': 0.15541758,\n"," 'Loss/total_loss': 0.45683515,\n"," 'learning_rate': 0.07995972}\n","INFO:tensorflow:Step 1800 per-step time 0.073s\n","I1226 13:21:13.227435 140232156051328 model_lib_v2.py:705] Step 1800 per-step time 0.073s\n","INFO:tensorflow:{'Loss/classification_loss': 0.18739401,\n"," 'Loss/localization_loss': 0.22892787,\n"," 'Loss/regularization_loss': 0.155204,\n"," 'Loss/total_loss': 0.5715259,\n"," 'learning_rate': 0.0799474}\n","I1226 13:21:13.227725 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.18739401,\n"," 'Loss/localization_loss': 0.22892787,\n"," 'Loss/regularization_loss': 0.155204,\n"," 'Loss/total_loss': 0.5715259,\n"," 'learning_rate': 0.0799474}\n","INFO:tensorflow:Step 1900 per-step time 0.072s\n","I1226 13:21:20.461373 140232156051328 model_lib_v2.py:705] Step 1900 per-step time 0.072s\n","INFO:tensorflow:{'Loss/classification_loss': 0.117589496,\n"," 'Loss/localization_loss': 0.0984338,\n"," 'Loss/regularization_loss': 0.15490507,\n"," 'Loss/total_loss': 0.37092835,\n"," 'learning_rate': 0.07993342}\n","I1226 13:21:20.461702 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.117589496,\n"," 'Loss/localization_loss': 0.0984338,\n"," 'Loss/regularization_loss': 0.15490507,\n"," 'Loss/total_loss': 0.37092835,\n"," 'learning_rate': 0.07993342}\n","INFO:tensorflow:Step 2000 per-step time 0.073s\n","I1226 13:21:27.790665 140232156051328 model_lib_v2.py:705] Step 2000 per-step time 0.073s\n","INFO:tensorflow:{'Loss/classification_loss': 0.22600806,\n"," 'Loss/localization_loss': 0.17298788,\n"," 'Loss/regularization_loss': 0.15445898,\n"," 'Loss/total_loss': 0.55345494,\n"," 'learning_rate': 0.07991781}\n","I1226 13:21:27.790944 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.22600806,\n"," 'Loss/localization_loss': 0.17298788,\n"," 'Loss/regularization_loss': 0.15445898,\n"," 'Loss/total_loss': 0.55345494,\n"," 'learning_rate': 0.07991781}\n","INFO:tensorflow:Step 2100 per-step time 0.085s\n","I1226 13:21:36.342680 140232156051328 model_lib_v2.py:705] Step 2100 per-step time 0.085s\n","INFO:tensorflow:{'Loss/classification_loss': 0.22327131,\n"," 'Loss/localization_loss': 0.18911561,\n"," 'Loss/regularization_loss': 0.15417978,\n"," 'Loss/total_loss': 0.5665667,\n"," 'learning_rate': 0.07990056}\n","I1226 13:21:36.342964 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.22327131,\n"," 'Loss/localization_loss': 0.18911561,\n"," 'Loss/regularization_loss': 0.15417978,\n"," 'Loss/total_loss': 0.5665667,\n"," 'learning_rate': 0.07990056}\n","INFO:tensorflow:Step 2200 per-step time 0.074s\n","I1226 13:21:43.765759 140232156051328 model_lib_v2.py:705] Step 2200 per-step time 0.074s\n","INFO:tensorflow:{'Loss/classification_loss': 0.1706182,\n"," 'Loss/localization_loss': 0.14487076,\n"," 'Loss/regularization_loss': 0.1540391,\n"," 'Loss/total_loss': 0.46952808,\n"," 'learning_rate': 0.07988167}\n","I1226 13:21:43.766036 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.1706182,\n"," 'Loss/localization_loss': 0.14487076,\n"," 'Loss/regularization_loss': 0.1540391,\n"," 'Loss/total_loss': 0.46952808,\n"," 'learning_rate': 0.07988167}\n","INFO:tensorflow:Step 2300 per-step time 0.074s\n","I1226 13:21:51.166426 140232156051328 model_lib_v2.py:705] Step 2300 per-step time 0.074s\n","INFO:tensorflow:{'Loss/classification_loss': 0.151151,\n"," 'Loss/localization_loss': 0.108047724,\n"," 'Loss/regularization_loss': 0.15375365,\n"," 'Loss/total_loss': 0.41295236,\n"," 'learning_rate': 0.07986114}\n","I1226 13:21:51.166713 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.151151,\n"," 'Loss/localization_loss': 0.108047724,\n"," 'Loss/regularization_loss': 0.15375365,\n"," 'Loss/total_loss': 0.41295236,\n"," 'learning_rate': 0.07986114}\n","INFO:tensorflow:Step 2400 per-step time 0.073s\n","I1226 13:21:58.495986 140232156051328 model_lib_v2.py:705] Step 2400 per-step time 0.073s\n","INFO:tensorflow:{'Loss/classification_loss': 0.16424513,\n"," 'Loss/localization_loss': 0.14768197,\n"," 'Loss/regularization_loss': 0.15341324,\n"," 'Loss/total_loss': 0.46534032,\n"," 'learning_rate': 0.07983897}\n","I1226 13:21:58.496304 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.16424513,\n"," 'Loss/localization_loss': 0.14768197,\n"," 'Loss/regularization_loss': 0.15341324,\n"," 'Loss/total_loss': 0.46534032,\n"," 'learning_rate': 0.07983897}\n","INFO:tensorflow:Step 2500 per-step time 0.073s\n","I1226 13:22:05.757445 140232156051328 model_lib_v2.py:705] Step 2500 per-step time 0.073s\n","INFO:tensorflow:{'Loss/classification_loss': 0.15084107,\n"," 'Loss/localization_loss': 0.15796551,\n"," 'Loss/regularization_loss': 0.15331092,\n"," 'Loss/total_loss': 0.46211752,\n"," 'learning_rate': 0.079815164}\n","I1226 13:22:05.757726 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.15084107,\n"," 'Loss/localization_loss': 0.15796551,\n"," 'Loss/regularization_loss': 0.15331092,\n"," 'Loss/total_loss': 0.46211752,\n"," 'learning_rate': 0.079815164}\n","INFO:tensorflow:Step 2600 per-step time 0.072s\n","I1226 13:22:12.986576 140232156051328 model_lib_v2.py:705] Step 2600 per-step time 0.072s\n","INFO:tensorflow:{'Loss/classification_loss': 0.21917374,\n"," 'Loss/localization_loss': 0.2214299,\n"," 'Loss/regularization_loss': 0.15340877,\n"," 'Loss/total_loss': 0.5940124,\n"," 'learning_rate': 0.07978972}\n","I1226 13:22:12.986918 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.21917374,\n"," 'Loss/localization_loss': 0.2214299,\n"," 'Loss/regularization_loss': 0.15340877,\n"," 'Loss/total_loss': 0.5940124,\n"," 'learning_rate': 0.07978972}\n","INFO:tensorflow:Step 2700 per-step time 0.116s\n","I1226 13:22:24.547464 140232156051328 model_lib_v2.py:705] Step 2700 per-step time 0.116s\n","INFO:tensorflow:{'Loss/classification_loss': 0.27903286,\n"," 'Loss/localization_loss': 0.39706555,\n"," 'Loss/regularization_loss': 0.15324387,\n"," 'Loss/total_loss': 0.82934225,\n"," 'learning_rate': 0.07976264}\n","I1226 13:22:24.547818 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.27903286,\n"," 'Loss/localization_loss': 0.39706555,\n"," 'Loss/regularization_loss': 0.15324387,\n"," 'Loss/total_loss': 0.82934225,\n"," 'learning_rate': 0.07976264}\n","INFO:tensorflow:Step 2800 per-step time 0.074s\n","I1226 13:22:31.975263 140232156051328 model_lib_v2.py:705] Step 2800 per-step time 0.074s\n","INFO:tensorflow:{'Loss/classification_loss': 0.22701558,\n"," 'Loss/localization_loss': 0.16427499,\n"," 'Loss/regularization_loss': 0.15292685,\n"," 'Loss/total_loss': 0.5442174,\n"," 'learning_rate': 0.07973392}\n","I1226 13:22:31.975543 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.22701558,\n"," 'Loss/localization_loss': 0.16427499,\n"," 'Loss/regularization_loss': 0.15292685,\n"," 'Loss/total_loss': 0.5442174,\n"," 'learning_rate': 0.07973392}\n","INFO:tensorflow:Step 2900 per-step time 0.074s\n","I1226 13:22:39.415921 140232156051328 model_lib_v2.py:705] Step 2900 per-step time 0.074s\n","INFO:tensorflow:{'Loss/classification_loss': 0.2063165,\n"," 'Loss/localization_loss': 0.1824617,\n"," 'Loss/regularization_loss': 0.15263426,\n"," 'Loss/total_loss': 0.5414125,\n"," 'learning_rate': 0.07970358}\n","I1226 13:22:39.416288 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.2063165,\n"," 'Loss/localization_loss': 0.1824617,\n"," 'Loss/regularization_loss': 0.15263426,\n"," 'Loss/total_loss': 0.5414125,\n"," 'learning_rate': 0.07970358}\n","INFO:tensorflow:Step 3000 per-step time 0.073s\n","I1226 13:22:46.680847 140232156051328 model_lib_v2.py:705] Step 3000 per-step time 0.073s\n","INFO:tensorflow:{'Loss/classification_loss': 0.11750338,\n"," 'Loss/localization_loss': 0.13700719,\n"," 'Loss/regularization_loss': 0.15210997,\n"," 'Loss/total_loss': 0.40662056,\n"," 'learning_rate': 0.0796716}\n","I1226 13:22:46.681140 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.11750338,\n"," 'Loss/localization_loss': 0.13700719,\n"," 'Loss/regularization_loss': 0.15210997,\n"," 'Loss/total_loss': 0.40662056,\n"," 'learning_rate': 0.0796716}\n","INFO:tensorflow:Step 3100 per-step time 0.085s\n","I1226 13:22:55.199131 140232156051328 model_lib_v2.py:705] Step 3100 per-step time 0.085s\n","INFO:tensorflow:{'Loss/classification_loss': 0.11156801,\n"," 'Loss/localization_loss': 0.055772606,\n"," 'Loss/regularization_loss': 0.1518945,\n"," 'Loss/total_loss': 0.31923512,\n"," 'learning_rate': 0.07963799}\n","I1226 13:22:55.199433 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.11156801,\n"," 'Loss/localization_loss': 0.055772606,\n"," 'Loss/regularization_loss': 0.1518945,\n"," 'Loss/total_loss': 0.31923512,\n"," 'learning_rate': 0.07963799}\n","INFO:tensorflow:Step 3200 per-step time 0.073s\n","I1226 13:23:02.484892 140232156051328 model_lib_v2.py:705] Step 3200 per-step time 0.073s\n","INFO:tensorflow:{'Loss/classification_loss': 0.14743295,\n"," 'Loss/localization_loss': 0.16931282,\n"," 'Loss/regularization_loss': 0.15150028,\n"," 'Loss/total_loss': 0.46824604,\n"," 'learning_rate': 0.07960275}\n","I1226 13:23:02.485198 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.14743295,\n"," 'Loss/localization_loss': 0.16931282,\n"," 'Loss/regularization_loss': 0.15150028,\n"," 'Loss/total_loss': 0.46824604,\n"," 'learning_rate': 0.07960275}\n","INFO:tensorflow:Step 3300 per-step time 0.071s\n","I1226 13:23:09.576564 140232156051328 model_lib_v2.py:705] Step 3300 per-step time 0.071s\n","INFO:tensorflow:{'Loss/classification_loss': 0.13972689,\n"," 'Loss/localization_loss': 0.077164866,\n"," 'Loss/regularization_loss': 0.1511987,\n"," 'Loss/total_loss': 0.36809045,\n"," 'learning_rate': 0.07956588}\n","I1226 13:23:09.576870 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.13972689,\n"," 'Loss/localization_loss': 0.077164866,\n"," 'Loss/regularization_loss': 0.1511987,\n"," 'Loss/total_loss': 0.36809045,\n"," 'learning_rate': 0.07956588}\n","INFO:tensorflow:Step 3400 per-step time 0.073s\n","I1226 13:23:16.879766 140232156051328 model_lib_v2.py:705] Step 3400 per-step time 0.073s\n","INFO:tensorflow:{'Loss/classification_loss': 0.2555836,\n"," 'Loss/localization_loss': 0.20869194,\n"," 'Loss/regularization_loss': 0.15073441,\n"," 'Loss/total_loss': 0.61500996,\n"," 'learning_rate': 0.079527386}\n","I1226 13:23:16.880067 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.2555836,\n"," 'Loss/localization_loss': 0.20869194,\n"," 'Loss/regularization_loss': 0.15073441,\n"," 'Loss/total_loss': 0.61500996,\n"," 'learning_rate': 0.079527386}\n","INFO:tensorflow:Step 3500 per-step time 0.072s\n","I1226 13:23:24.124830 140232156051328 model_lib_v2.py:705] Step 3500 per-step time 0.072s\n","INFO:tensorflow:{'Loss/classification_loss': 0.1470075,\n"," 'Loss/localization_loss': 0.071046256,\n"," 'Loss/regularization_loss': 0.15059789,\n"," 'Loss/total_loss': 0.36865163,\n"," 'learning_rate': 0.07948727}\n","I1226 13:23:24.125104 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.1470075,\n"," 'Loss/localization_loss': 0.071046256,\n"," 'Loss/regularization_loss': 0.15059789,\n"," 'Loss/total_loss': 0.36865163,\n"," 'learning_rate': 0.07948727}\n","INFO:tensorflow:Step 3600 per-step time 0.072s\n","I1226 13:23:31.369449 140232156051328 model_lib_v2.py:705] Step 3600 per-step time 0.072s\n","INFO:tensorflow:{'Loss/classification_loss': 0.19916119,\n"," 'Loss/localization_loss': 0.3189077,\n"," 'Loss/regularization_loss': 0.15009555,\n"," 'Loss/total_loss': 0.6681645,\n"," 'learning_rate': 0.079445526}\n","I1226 13:23:31.369768 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.19916119,\n"," 'Loss/localization_loss': 0.3189077,\n"," 'Loss/regularization_loss': 0.15009555,\n"," 'Loss/total_loss': 0.6681645,\n"," 'learning_rate': 0.079445526}\n","INFO:tensorflow:Step 3700 per-step time 0.073s\n","I1226 13:23:38.649203 140232156051328 model_lib_v2.py:705] Step 3700 per-step time 0.073s\n","INFO:tensorflow:{'Loss/classification_loss': 0.12796514,\n"," 'Loss/localization_loss': 0.10713746,\n"," 'Loss/regularization_loss': 0.14952536,\n"," 'Loss/total_loss': 0.38462794,\n"," 'learning_rate': 0.07940216}\n","I1226 13:23:38.649523 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.12796514,\n"," 'Loss/localization_loss': 0.10713746,\n"," 'Loss/regularization_loss': 0.14952536,\n"," 'Loss/total_loss': 0.38462794,\n"," 'learning_rate': 0.07940216}\n","INFO:tensorflow:Step 3800 per-step time 0.073s\n","I1226 13:23:45.976023 140232156051328 model_lib_v2.py:705] Step 3800 per-step time 0.073s\n","INFO:tensorflow:{'Loss/classification_loss': 0.16790475,\n"," 'Loss/localization_loss': 0.14998391,\n"," 'Loss/regularization_loss': 0.14902247,\n"," 'Loss/total_loss': 0.46691114,\n"," 'learning_rate': 0.079357184}\n","I1226 13:23:45.976439 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.16790475,\n"," 'Loss/localization_loss': 0.14998391,\n"," 'Loss/regularization_loss': 0.14902247,\n"," 'Loss/total_loss': 0.46691114,\n"," 'learning_rate': 0.079357184}\n","INFO:tensorflow:Step 3900 per-step time 0.075s\n","I1226 13:23:53.450194 140232156051328 model_lib_v2.py:705] Step 3900 per-step time 0.075s\n","INFO:tensorflow:{'Loss/classification_loss': 0.14462213,\n"," 'Loss/localization_loss': 0.060118668,\n"," 'Loss/regularization_loss': 0.14867795,\n"," 'Loss/total_loss': 0.35341874,\n"," 'learning_rate': 0.07931058}\n","I1226 13:23:53.450545 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.14462213,\n"," 'Loss/localization_loss': 0.060118668,\n"," 'Loss/regularization_loss': 0.14867795,\n"," 'Loss/total_loss': 0.35341874,\n"," 'learning_rate': 0.07931058}\n","INFO:tensorflow:Step 4000 per-step time 0.073s\n","I1226 13:24:00.704643 140232156051328 model_lib_v2.py:705] Step 4000 per-step time 0.073s\n","INFO:tensorflow:{'Loss/classification_loss': 0.15822503,\n"," 'Loss/localization_loss': 0.15387112,\n"," 'Loss/regularization_loss': 0.14828096,\n"," 'Loss/total_loss': 0.4603771,\n"," 'learning_rate': 0.07926236}\n","I1226 13:24:00.704962 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.15822503,\n"," 'Loss/localization_loss': 0.15387112,\n"," 'Loss/regularization_loss': 0.14828096,\n"," 'Loss/total_loss': 0.4603771,\n"," 'learning_rate': 0.07926236}\n","INFO:tensorflow:Step 4100 per-step time 0.086s\n","I1226 13:24:09.326283 140232156051328 model_lib_v2.py:705] Step 4100 per-step time 0.086s\n","INFO:tensorflow:{'Loss/classification_loss': 0.15807746,\n"," 'Loss/localization_loss': 0.11136733,\n"," 'Loss/regularization_loss': 0.14775982,\n"," 'Loss/total_loss': 0.41720462,\n"," 'learning_rate': 0.07921253}\n","I1226 13:24:09.326604 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.15807746,\n"," 'Loss/localization_loss': 0.11136733,\n"," 'Loss/regularization_loss': 0.14775982,\n"," 'Loss/total_loss': 0.41720462,\n"," 'learning_rate': 0.07921253}\n","INFO:tensorflow:Step 4200 per-step time 0.071s\n","I1226 13:24:16.452673 140232156051328 model_lib_v2.py:705] Step 4200 per-step time 0.071s\n","INFO:tensorflow:{'Loss/classification_loss': 0.11109664,\n"," 'Loss/localization_loss': 0.083329394,\n"," 'Loss/regularization_loss': 0.14725505,\n"," 'Loss/total_loss': 0.34168106,\n"," 'learning_rate': 0.07916109}\n","I1226 13:24:16.452981 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.11109664,\n"," 'Loss/localization_loss': 0.083329394,\n"," 'Loss/regularization_loss': 0.14725505,\n"," 'Loss/total_loss': 0.34168106,\n"," 'learning_rate': 0.07916109}\n","INFO:tensorflow:Step 4300 per-step time 0.074s\n","I1226 13:24:23.832475 140232156051328 model_lib_v2.py:705] Step 4300 per-step time 0.074s\n","INFO:tensorflow:{'Loss/classification_loss': 0.094757386,\n"," 'Loss/localization_loss': 0.08520981,\n"," 'Loss/regularization_loss': 0.14673108,\n"," 'Loss/total_loss': 0.32669827,\n"," 'learning_rate': 0.07910804}\n","I1226 13:24:23.832820 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.094757386,\n"," 'Loss/localization_loss': 0.08520981,\n"," 'Loss/regularization_loss': 0.14673108,\n"," 'Loss/total_loss': 0.32669827,\n"," 'learning_rate': 0.07910804}\n","INFO:tensorflow:Step 4400 per-step time 0.075s\n","I1226 13:24:31.356465 140232156051328 model_lib_v2.py:705] Step 4400 per-step time 0.075s\n","INFO:tensorflow:{'Loss/classification_loss': 0.18554917,\n"," 'Loss/localization_loss': 0.1510243,\n"," 'Loss/regularization_loss': 0.14615537,\n"," 'Loss/total_loss': 0.48272884,\n"," 'learning_rate': 0.07905338}\n","I1226 13:24:31.356743 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.18554917,\n"," 'Loss/localization_loss': 0.1510243,\n"," 'Loss/regularization_loss': 0.14615537,\n"," 'Loss/total_loss': 0.48272884,\n"," 'learning_rate': 0.07905338}\n","INFO:tensorflow:Step 4500 per-step time 0.075s\n","I1226 13:24:38.843685 140232156051328 model_lib_v2.py:705] Step 4500 per-step time 0.075s\n","INFO:tensorflow:{'Loss/classification_loss': 0.08193532,\n"," 'Loss/localization_loss': 0.04854562,\n"," 'Loss/regularization_loss': 0.1456993,\n"," 'Loss/total_loss': 0.27618027,\n"," 'learning_rate': 0.07899711}\n","I1226 13:24:38.844105 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.08193532,\n"," 'Loss/localization_loss': 0.04854562,\n"," 'Loss/regularization_loss': 0.1456993,\n"," 'Loss/total_loss': 0.27618027,\n"," 'learning_rate': 0.07899711}\n","INFO:tensorflow:Step 4600 per-step time 0.072s\n","I1226 13:24:46.081062 140232156051328 model_lib_v2.py:705] Step 4600 per-step time 0.072s\n","INFO:tensorflow:{'Loss/classification_loss': 0.26797882,\n"," 'Loss/localization_loss': 0.10487742,\n"," 'Loss/regularization_loss': 0.14521202,\n"," 'Loss/total_loss': 0.51806825,\n"," 'learning_rate': 0.078939244}\n","I1226 13:24:46.081366 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.26797882,\n"," 'Loss/localization_loss': 0.10487742,\n"," 'Loss/regularization_loss': 0.14521202,\n"," 'Loss/total_loss': 0.51806825,\n"," 'learning_rate': 0.078939244}\n","INFO:tensorflow:Step 4700 per-step time 0.073s\n","I1226 13:24:53.375133 140232156051328 model_lib_v2.py:705] Step 4700 per-step time 0.073s\n","INFO:tensorflow:{'Loss/classification_loss': 0.11898425,\n"," 'Loss/localization_loss': 0.093007885,\n"," 'Loss/regularization_loss': 0.14468867,\n"," 'Loss/total_loss': 0.3566808,\n"," 'learning_rate': 0.07887978}\n","I1226 13:24:53.375433 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.11898425,\n"," 'Loss/localization_loss': 0.093007885,\n"," 'Loss/regularization_loss': 0.14468867,\n"," 'Loss/total_loss': 0.3566808,\n"," 'learning_rate': 0.07887978}\n","INFO:tensorflow:Step 4800 per-step time 0.073s\n","I1226 13:25:00.711678 140232156051328 model_lib_v2.py:705] Step 4800 per-step time 0.073s\n","INFO:tensorflow:{'Loss/classification_loss': 0.11686617,\n"," 'Loss/localization_loss': 0.1291645,\n"," 'Loss/regularization_loss': 0.14423048,\n"," 'Loss/total_loss': 0.39026117,\n"," 'learning_rate': 0.07881871}\n","I1226 13:25:00.711968 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.11686617,\n"," 'Loss/localization_loss': 0.1291645,\n"," 'Loss/regularization_loss': 0.14423048,\n"," 'Loss/total_loss': 0.39026117,\n"," 'learning_rate': 0.07881871}\n","INFO:tensorflow:Step 4900 per-step time 0.075s\n","I1226 13:25:08.255606 140232156051328 model_lib_v2.py:705] Step 4900 per-step time 0.075s\n","INFO:tensorflow:{'Loss/classification_loss': 0.11244199,\n"," 'Loss/localization_loss': 0.048885725,\n"," 'Loss/regularization_loss': 0.14373016,\n"," 'Loss/total_loss': 0.30505788,\n"," 'learning_rate': 0.07875605}\n","I1226 13:25:08.255954 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.11244199,\n"," 'Loss/localization_loss': 0.048885725,\n"," 'Loss/regularization_loss': 0.14373016,\n"," 'Loss/total_loss': 0.30505788,\n"," 'learning_rate': 0.07875605}\n","INFO:tensorflow:Step 5000 per-step time 0.116s\n","I1226 13:25:19.827567 140232156051328 model_lib_v2.py:705] Step 5000 per-step time 0.116s\n","INFO:tensorflow:{'Loss/classification_loss': 0.1650215,\n"," 'Loss/localization_loss': 0.14048284,\n"," 'Loss/regularization_loss': 0.14322929,\n"," 'Loss/total_loss': 0.44873363,\n"," 'learning_rate': 0.078691795}\n","I1226 13:25:19.827928 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.1650215,\n"," 'Loss/localization_loss': 0.14048284,\n"," 'Loss/regularization_loss': 0.14322929,\n"," 'Loss/total_loss': 0.44873363,\n"," 'learning_rate': 0.078691795}\n","INFO:tensorflow:Step 5100 per-step time 0.087s\n","I1226 13:25:28.493776 140232156051328 model_lib_v2.py:705] Step 5100 per-step time 0.087s\n","INFO:tensorflow:{'Loss/classification_loss': 0.12755927,\n"," 'Loss/localization_loss': 0.122212805,\n"," 'Loss/regularization_loss': 0.14281072,\n"," 'Loss/total_loss': 0.39258277,\n"," 'learning_rate': 0.07862595}\n","I1226 13:25:28.494070 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.12755927,\n"," 'Loss/localization_loss': 0.122212805,\n"," 'Loss/regularization_loss': 0.14281072,\n"," 'Loss/total_loss': 0.39258277,\n"," 'learning_rate': 0.07862595}\n","INFO:tensorflow:Step 5200 per-step time 0.074s\n","I1226 13:25:35.851335 140232156051328 model_lib_v2.py:705] Step 5200 per-step time 0.074s\n","INFO:tensorflow:{'Loss/classification_loss': 0.19318818,\n"," 'Loss/localization_loss': 0.117888436,\n"," 'Loss/regularization_loss': 0.14231062,\n"," 'Loss/total_loss': 0.45338723,\n"," 'learning_rate': 0.07855851}\n","I1226 13:25:35.851608 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.19318818,\n"," 'Loss/localization_loss': 0.117888436,\n"," 'Loss/regularization_loss': 0.14231062,\n"," 'Loss/total_loss': 0.45338723,\n"," 'learning_rate': 0.07855851}\n","INFO:tensorflow:Step 5300 per-step time 0.075s\n","I1226 13:25:43.322365 140232156051328 model_lib_v2.py:705] Step 5300 per-step time 0.075s\n","INFO:tensorflow:{'Loss/classification_loss': 0.11611448,\n"," 'Loss/localization_loss': 0.095348485,\n"," 'Loss/regularization_loss': 0.14197731,\n"," 'Loss/total_loss': 0.35344028,\n"," 'learning_rate': 0.07848949}\n","I1226 13:25:43.322669 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.11611448,\n"," 'Loss/localization_loss': 0.095348485,\n"," 'Loss/regularization_loss': 0.14197731,\n"," 'Loss/total_loss': 0.35344028,\n"," 'learning_rate': 0.07848949}\n","INFO:tensorflow:Step 5400 per-step time 0.071s\n","I1226 13:25:50.456908 140232156051328 model_lib_v2.py:705] Step 5400 per-step time 0.071s\n","INFO:tensorflow:{'Loss/classification_loss': 0.08760539,\n"," 'Loss/localization_loss': 0.06961804,\n"," 'Loss/regularization_loss': 0.14149442,\n"," 'Loss/total_loss': 0.29871786,\n"," 'learning_rate': 0.078418896}\n","I1226 13:25:50.457199 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.08760539,\n"," 'Loss/localization_loss': 0.06961804,\n"," 'Loss/regularization_loss': 0.14149442,\n"," 'Loss/total_loss': 0.29871786,\n"," 'learning_rate': 0.078418896}\n","INFO:tensorflow:Step 5500 per-step time 0.073s\n","I1226 13:25:57.721718 140232156051328 model_lib_v2.py:705] Step 5500 per-step time 0.073s\n","INFO:tensorflow:{'Loss/classification_loss': 0.2575274,\n"," 'Loss/localization_loss': 0.06291755,\n"," 'Loss/regularization_loss': 0.14093849,\n"," 'Loss/total_loss': 0.46138346,\n"," 'learning_rate': 0.078346714}\n","I1226 13:25:57.722017 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.2575274,\n"," 'Loss/localization_loss': 0.06291755,\n"," 'Loss/regularization_loss': 0.14093849,\n"," 'Loss/total_loss': 0.46138346,\n"," 'learning_rate': 0.078346714}\n","INFO:tensorflow:Step 5600 per-step time 0.074s\n","I1226 13:26:05.093807 140232156051328 model_lib_v2.py:705] Step 5600 per-step time 0.074s\n","INFO:tensorflow:{'Loss/classification_loss': 0.115208685,\n"," 'Loss/localization_loss': 0.089387484,\n"," 'Loss/regularization_loss': 0.14044304,\n"," 'Loss/total_loss': 0.3450392,\n"," 'learning_rate': 0.07827295}\n","I1226 13:26:05.094088 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.115208685,\n"," 'Loss/localization_loss': 0.089387484,\n"," 'Loss/regularization_loss': 0.14044304,\n"," 'Loss/total_loss': 0.3450392,\n"," 'learning_rate': 0.07827295}\n","INFO:tensorflow:Step 5700 per-step time 0.072s\n","I1226 13:26:12.329487 140232156051328 model_lib_v2.py:705] Step 5700 per-step time 0.072s\n","INFO:tensorflow:{'Loss/classification_loss': 0.11337415,\n"," 'Loss/localization_loss': 0.08062327,\n"," 'Loss/regularization_loss': 0.1399139,\n"," 'Loss/total_loss': 0.3339113,\n"," 'learning_rate': 0.07819763}\n","I1226 13:26:12.329776 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.11337415,\n"," 'Loss/localization_loss': 0.08062327,\n"," 'Loss/regularization_loss': 0.1399139,\n"," 'Loss/total_loss': 0.3339113,\n"," 'learning_rate': 0.07819763}\n","INFO:tensorflow:Step 5800 per-step time 0.073s\n","I1226 13:26:19.620151 140232156051328 model_lib_v2.py:705] Step 5800 per-step time 0.073s\n","INFO:tensorflow:{'Loss/classification_loss': 0.09770166,\n"," 'Loss/localization_loss': 0.1310732,\n"," 'Loss/regularization_loss': 0.1394538,\n"," 'Loss/total_loss': 0.36822867,\n"," 'learning_rate': 0.07812072}\n","I1226 13:26:19.620447 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.09770166,\n"," 'Loss/localization_loss': 0.1310732,\n"," 'Loss/regularization_loss': 0.1394538,\n"," 'Loss/total_loss': 0.36822867,\n"," 'learning_rate': 0.07812072}\n","INFO:tensorflow:Step 5900 per-step time 0.072s\n","I1226 13:26:26.791421 140232156051328 model_lib_v2.py:705] Step 5900 per-step time 0.072s\n","INFO:tensorflow:{'Loss/classification_loss': 0.18492784,\n"," 'Loss/localization_loss': 0.20038232,\n"," 'Loss/regularization_loss': 0.13910572,\n"," 'Loss/total_loss': 0.5244159,\n"," 'learning_rate': 0.078042254}\n","I1226 13:26:26.791699 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.18492784,\n"," 'Loss/localization_loss': 0.20038232,\n"," 'Loss/regularization_loss': 0.13910572,\n"," 'Loss/total_loss': 0.5244159,\n"," 'learning_rate': 0.078042254}\n","INFO:tensorflow:Step 6000 per-step time 0.072s\n","I1226 13:26:34.049942 140232156051328 model_lib_v2.py:705] Step 6000 per-step time 0.072s\n","INFO:tensorflow:{'Loss/classification_loss': 0.15570955,\n"," 'Loss/localization_loss': 0.15631689,\n"," 'Loss/regularization_loss': 0.13863538,\n"," 'Loss/total_loss': 0.45066184,\n"," 'learning_rate': 0.07796223}\n","I1226 13:26:34.050364 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.15570955,\n"," 'Loss/localization_loss': 0.15631689,\n"," 'Loss/regularization_loss': 0.13863538,\n"," 'Loss/total_loss': 0.45066184,\n"," 'learning_rate': 0.07796223}\n","INFO:tensorflow:Step 6100 per-step time 0.086s\n","I1226 13:26:42.662553 140232156051328 model_lib_v2.py:705] Step 6100 per-step time 0.086s\n","INFO:tensorflow:{'Loss/classification_loss': 0.13334422,\n"," 'Loss/localization_loss': 0.07875082,\n"," 'Loss/regularization_loss': 0.1381337,\n"," 'Loss/total_loss': 0.35022873,\n"," 'learning_rate': 0.077880636}\n","I1226 13:26:42.662853 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.13334422,\n"," 'Loss/localization_loss': 0.07875082,\n"," 'Loss/regularization_loss': 0.1381337,\n"," 'Loss/total_loss': 0.35022873,\n"," 'learning_rate': 0.077880636}\n","INFO:tensorflow:Step 6200 per-step time 0.073s\n","I1226 13:26:49.969564 140232156051328 model_lib_v2.py:705] Step 6200 per-step time 0.073s\n","INFO:tensorflow:{'Loss/classification_loss': 0.13762751,\n"," 'Loss/localization_loss': 0.0869486,\n"," 'Loss/regularization_loss': 0.1377736,\n"," 'Loss/total_loss': 0.36234972,\n"," 'learning_rate': 0.07779749}\n","I1226 13:26:49.969846 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.13762751,\n"," 'Loss/localization_loss': 0.0869486,\n"," 'Loss/regularization_loss': 0.1377736,\n"," 'Loss/total_loss': 0.36234972,\n"," 'learning_rate': 0.07779749}\n","INFO:tensorflow:Step 6300 per-step time 0.073s\n","I1226 13:26:57.222667 140232156051328 model_lib_v2.py:705] Step 6300 per-step time 0.073s\n","INFO:tensorflow:{'Loss/classification_loss': 0.12973101,\n"," 'Loss/localization_loss': 0.13557467,\n"," 'Loss/regularization_loss': 0.13731106,\n"," 'Loss/total_loss': 0.40261674,\n"," 'learning_rate': 0.07771279}\n","I1226 13:26:57.222949 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.12973101,\n"," 'Loss/localization_loss': 0.13557467,\n"," 'Loss/regularization_loss': 0.13731106,\n"," 'Loss/total_loss': 0.40261674,\n"," 'learning_rate': 0.07771279}\n","INFO:tensorflow:Step 6400 per-step time 0.071s\n","I1226 13:27:04.369205 140232156051328 model_lib_v2.py:705] Step 6400 per-step time 0.071s\n","INFO:tensorflow:{'Loss/classification_loss': 0.14079335,\n"," 'Loss/localization_loss': 0.10789102,\n"," 'Loss/regularization_loss': 0.13687097,\n"," 'Loss/total_loss': 0.38555533,\n"," 'learning_rate': 0.077626534}\n","I1226 13:27:04.369505 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.14079335,\n"," 'Loss/localization_loss': 0.10789102,\n"," 'Loss/regularization_loss': 0.13687097,\n"," 'Loss/total_loss': 0.38555533,\n"," 'learning_rate': 0.077626534}\n","INFO:tensorflow:Step 6500 per-step time 0.075s\n","I1226 13:27:11.915633 140232156051328 model_lib_v2.py:705] Step 6500 per-step time 0.075s\n","INFO:tensorflow:{'Loss/classification_loss': 0.066352926,\n"," 'Loss/localization_loss': 0.052211005,\n"," 'Loss/regularization_loss': 0.13640845,\n"," 'Loss/total_loss': 0.2549724,\n"," 'learning_rate': 0.077538736}\n","I1226 13:27:11.915913 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.066352926,\n"," 'Loss/localization_loss': 0.052211005,\n"," 'Loss/regularization_loss': 0.13640845,\n"," 'Loss/total_loss': 0.2549724,\n"," 'learning_rate': 0.077538736}\n","INFO:tensorflow:Step 6600 per-step time 0.073s\n","I1226 13:27:19.235314 140232156051328 model_lib_v2.py:705] Step 6600 per-step time 0.073s\n","INFO:tensorflow:{'Loss/classification_loss': 0.11618992,\n"," 'Loss/localization_loss': 0.06686641,\n"," 'Loss/regularization_loss': 0.13598834,\n"," 'Loss/total_loss': 0.31904465,\n"," 'learning_rate': 0.077449396}\n","I1226 13:27:19.235662 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.11618992,\n"," 'Loss/localization_loss': 0.06686641,\n"," 'Loss/regularization_loss': 0.13598834,\n"," 'Loss/total_loss': 0.31904465,\n"," 'learning_rate': 0.077449396}\n","INFO:tensorflow:Step 6700 per-step time 0.073s\n","I1226 13:27:26.525370 140232156051328 model_lib_v2.py:705] Step 6700 per-step time 0.073s\n","INFO:tensorflow:{'Loss/classification_loss': 0.14495009,\n"," 'Loss/localization_loss': 0.07631006,\n"," 'Loss/regularization_loss': 0.13551745,\n"," 'Loss/total_loss': 0.3567776,\n"," 'learning_rate': 0.077358514}\n","I1226 13:27:26.525647 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.14495009,\n"," 'Loss/localization_loss': 0.07631006,\n"," 'Loss/regularization_loss': 0.13551745,\n"," 'Loss/total_loss': 0.3567776,\n"," 'learning_rate': 0.077358514}\n","INFO:tensorflow:Step 6800 per-step time 0.072s\n","I1226 13:27:33.776962 140232156051328 model_lib_v2.py:705] Step 6800 per-step time 0.072s\n","INFO:tensorflow:{'Loss/classification_loss': 0.11249601,\n"," 'Loss/localization_loss': 0.06831804,\n"," 'Loss/regularization_loss': 0.13501008,\n"," 'Loss/total_loss': 0.31582415,\n"," 'learning_rate': 0.0772661}\n","I1226 13:27:33.777294 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.11249601,\n"," 'Loss/localization_loss': 0.06831804,\n"," 'Loss/regularization_loss': 0.13501008,\n"," 'Loss/total_loss': 0.31582415,\n"," 'learning_rate': 0.0772661}\n","INFO:tensorflow:Step 6900 per-step time 0.074s\n","I1226 13:27:41.180205 140232156051328 model_lib_v2.py:705] Step 6900 per-step time 0.074s\n","INFO:tensorflow:{'Loss/classification_loss': 0.111028634,\n"," 'Loss/localization_loss': 0.1041668,\n"," 'Loss/regularization_loss': 0.1345074,\n"," 'Loss/total_loss': 0.34970284,\n"," 'learning_rate': 0.077172145}\n","I1226 13:27:41.180524 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.111028634,\n"," 'Loss/localization_loss': 0.1041668,\n"," 'Loss/regularization_loss': 0.1345074,\n"," 'Loss/total_loss': 0.34970284,\n"," 'learning_rate': 0.077172145}\n","INFO:tensorflow:Step 7000 per-step time 0.073s\n","I1226 13:27:48.507921 140232156051328 model_lib_v2.py:705] Step 7000 per-step time 0.073s\n","INFO:tensorflow:{'Loss/classification_loss': 0.11551816,\n"," 'Loss/localization_loss': 0.057042263,\n"," 'Loss/regularization_loss': 0.13401118,\n"," 'Loss/total_loss': 0.3065716,\n"," 'learning_rate': 0.07707667}\n","I1226 13:27:48.508204 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.11551816,\n"," 'Loss/localization_loss': 0.057042263,\n"," 'Loss/regularization_loss': 0.13401118,\n"," 'Loss/total_loss': 0.3065716,\n"," 'learning_rate': 0.07707667}\n","INFO:tensorflow:Step 7100 per-step time 0.088s\n","I1226 13:27:57.322979 140232156051328 model_lib_v2.py:705] Step 7100 per-step time 0.088s\n","INFO:tensorflow:{'Loss/classification_loss': 0.09387075,\n"," 'Loss/localization_loss': 0.06893749,\n"," 'Loss/regularization_loss': 0.1334836,\n"," 'Loss/total_loss': 0.29629183,\n"," 'learning_rate': 0.07697967}\n","I1226 13:27:57.323313 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.09387075,\n"," 'Loss/localization_loss': 0.06893749,\n"," 'Loss/regularization_loss': 0.1334836,\n"," 'Loss/total_loss': 0.29629183,\n"," 'learning_rate': 0.07697967}\n","INFO:tensorflow:Step 7200 per-step time 0.073s\n","I1226 13:28:04.661833 140232156051328 model_lib_v2.py:705] Step 7200 per-step time 0.073s\n","INFO:tensorflow:{'Loss/classification_loss': 0.17730716,\n"," 'Loss/localization_loss': 0.10953205,\n"," 'Loss/regularization_loss': 0.13306823,\n"," 'Loss/total_loss': 0.41990745,\n"," 'learning_rate': 0.07688115}\n","I1226 13:28:04.662114 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.17730716,\n"," 'Loss/localization_loss': 0.10953205,\n"," 'Loss/regularization_loss': 0.13306823,\n"," 'Loss/total_loss': 0.41990745,\n"," 'learning_rate': 0.07688115}\n","INFO:tensorflow:Step 7300 per-step time 0.116s\n","I1226 13:28:16.230238 140232156051328 model_lib_v2.py:705] Step 7300 per-step time 0.116s\n","INFO:tensorflow:{'Loss/classification_loss': 0.37631822,\n"," 'Loss/localization_loss': 0.32464543,\n"," 'Loss/regularization_loss': 0.13306363,\n"," 'Loss/total_loss': 0.83402723,\n"," 'learning_rate': 0.07678111}\n","I1226 13:28:16.230599 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.37631822,\n"," 'Loss/localization_loss': 0.32464543,\n"," 'Loss/regularization_loss': 0.13306363,\n"," 'Loss/total_loss': 0.83402723,\n"," 'learning_rate': 0.07678111}\n","INFO:tensorflow:Step 7400 per-step time 0.080s\n","I1226 13:28:24.217769 140232156051328 model_lib_v2.py:705] Step 7400 per-step time 0.080s\n","INFO:tensorflow:{'Loss/classification_loss': 0.1724914,\n"," 'Loss/localization_loss': 0.1328378,\n"," 'Loss/regularization_loss': 0.13302842,\n"," 'Loss/total_loss': 0.43835762,\n"," 'learning_rate': 0.076679565}\n","I1226 13:28:24.218102 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.1724914,\n"," 'Loss/localization_loss': 0.1328378,\n"," 'Loss/regularization_loss': 0.13302842,\n"," 'Loss/total_loss': 0.43835762,\n"," 'learning_rate': 0.076679565}\n","INFO:tensorflow:Step 7500 per-step time 0.075s\n","I1226 13:28:31.719668 140232156051328 model_lib_v2.py:705] Step 7500 per-step time 0.075s\n","INFO:tensorflow:{'Loss/classification_loss': 0.09807366,\n"," 'Loss/localization_loss': 0.055909835,\n"," 'Loss/regularization_loss': 0.13278079,\n"," 'Loss/total_loss': 0.2867643,\n"," 'learning_rate': 0.0765765}\n","I1226 13:28:31.719941 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.09807366,\n"," 'Loss/localization_loss': 0.055909835,\n"," 'Loss/regularization_loss': 0.13278079,\n"," 'Loss/total_loss': 0.2867643,\n"," 'learning_rate': 0.0765765}\n","INFO:tensorflow:Step 7600 per-step time 0.075s\n","I1226 13:28:39.201023 140232156051328 model_lib_v2.py:705] Step 7600 per-step time 0.075s\n","INFO:tensorflow:{'Loss/classification_loss': 0.09033508,\n"," 'Loss/localization_loss': 0.07478208,\n"," 'Loss/regularization_loss': 0.132511,\n"," 'Loss/total_loss': 0.29762816,\n"," 'learning_rate': 0.07647194}\n","I1226 13:28:39.201337 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.09033508,\n"," 'Loss/localization_loss': 0.07478208,\n"," 'Loss/regularization_loss': 0.132511,\n"," 'Loss/total_loss': 0.29762816,\n"," 'learning_rate': 0.07647194}\n","INFO:tensorflow:Step 7700 per-step time 0.073s\n","I1226 13:28:46.524584 140232156051328 model_lib_v2.py:705] Step 7700 per-step time 0.073s\n","INFO:tensorflow:{'Loss/classification_loss': 0.15406324,\n"," 'Loss/localization_loss': 0.1707224,\n"," 'Loss/regularization_loss': 0.1320981,\n"," 'Loss/total_loss': 0.45688373,\n"," 'learning_rate': 0.07636588}\n","I1226 13:28:46.524923 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.15406324,\n"," 'Loss/localization_loss': 0.1707224,\n"," 'Loss/regularization_loss': 0.1320981,\n"," 'Loss/total_loss': 0.45688373,\n"," 'learning_rate': 0.07636588}\n","INFO:tensorflow:Step 7800 per-step time 0.075s\n","I1226 13:28:54.058490 140232156051328 model_lib_v2.py:705] Step 7800 per-step time 0.075s\n","INFO:tensorflow:{'Loss/classification_loss': 0.14525491,\n"," 'Loss/localization_loss': 0.072447814,\n"," 'Loss/regularization_loss': 0.13184442,\n"," 'Loss/total_loss': 0.34954715,\n"," 'learning_rate': 0.07625833}\n","I1226 13:28:54.058771 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.14525491,\n"," 'Loss/localization_loss': 0.072447814,\n"," 'Loss/regularization_loss': 0.13184442,\n"," 'Loss/total_loss': 0.34954715,\n"," 'learning_rate': 0.07625833}\n","INFO:tensorflow:Step 7900 per-step time 0.073s\n","I1226 13:29:01.373515 140232156051328 model_lib_v2.py:705] Step 7900 per-step time 0.073s\n","INFO:tensorflow:{'Loss/classification_loss': 0.09367512,\n"," 'Loss/localization_loss': 0.044381678,\n"," 'Loss/regularization_loss': 0.13145724,\n"," 'Loss/total_loss': 0.26951402,\n"," 'learning_rate': 0.07614928}\n","I1226 13:29:01.373808 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.09367512,\n"," 'Loss/localization_loss': 0.044381678,\n"," 'Loss/regularization_loss': 0.13145724,\n"," 'Loss/total_loss': 0.26951402,\n"," 'learning_rate': 0.07614928}\n","INFO:tensorflow:Step 8000 per-step time 0.074s\n","I1226 13:29:08.754008 140232156051328 model_lib_v2.py:705] Step 8000 per-step time 0.074s\n","INFO:tensorflow:{'Loss/classification_loss': 0.08894122,\n"," 'Loss/localization_loss': 0.07319872,\n"," 'Loss/regularization_loss': 0.13096465,\n"," 'Loss/total_loss': 0.2931046,\n"," 'learning_rate': 0.07603875}\n","I1226 13:29:08.754340 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.08894122,\n"," 'Loss/localization_loss': 0.07319872,\n"," 'Loss/regularization_loss': 0.13096465,\n"," 'Loss/total_loss': 0.2931046,\n"," 'learning_rate': 0.07603875}\n","INFO:tensorflow:Step 8100 per-step time 0.085s\n","I1226 13:29:17.211942 140232156051328 model_lib_v2.py:705] Step 8100 per-step time 0.085s\n","INFO:tensorflow:{'Loss/classification_loss': 0.123826325,\n"," 'Loss/localization_loss': 0.07523528,\n"," 'Loss/regularization_loss': 0.13049473,\n"," 'Loss/total_loss': 0.32955635,\n"," 'learning_rate': 0.07592674}\n","I1226 13:29:17.212254 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.123826325,\n"," 'Loss/localization_loss': 0.07523528,\n"," 'Loss/regularization_loss': 0.13049473,\n"," 'Loss/total_loss': 0.32955635,\n"," 'learning_rate': 0.07592674}\n","INFO:tensorflow:Step 8200 per-step time 0.075s\n","I1226 13:29:24.705576 140232156051328 model_lib_v2.py:705] Step 8200 per-step time 0.075s\n","INFO:tensorflow:{'Loss/classification_loss': 0.086775,\n"," 'Loss/localization_loss': 0.07832743,\n"," 'Loss/regularization_loss': 0.13004385,\n"," 'Loss/total_loss': 0.2951463,\n"," 'learning_rate': 0.075813256}\n","I1226 13:29:24.705909 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.086775,\n"," 'Loss/localization_loss': 0.07832743,\n"," 'Loss/regularization_loss': 0.13004385,\n"," 'Loss/total_loss': 0.2951463,\n"," 'learning_rate': 0.075813256}\n","INFO:tensorflow:Step 8300 per-step time 0.074s\n","I1226 13:29:32.064104 140232156051328 model_lib_v2.py:705] Step 8300 per-step time 0.074s\n","INFO:tensorflow:{'Loss/classification_loss': 0.19644146,\n"," 'Loss/localization_loss': 0.049646374,\n"," 'Loss/regularization_loss': 0.12959673,\n"," 'Loss/total_loss': 0.37568456,\n"," 'learning_rate': 0.07569829}\n","I1226 13:29:32.064406 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.19644146,\n"," 'Loss/localization_loss': 0.049646374,\n"," 'Loss/regularization_loss': 0.12959673,\n"," 'Loss/total_loss': 0.37568456,\n"," 'learning_rate': 0.07569829}\n","INFO:tensorflow:Step 8400 per-step time 0.075s\n","I1226 13:29:39.615941 140232156051328 model_lib_v2.py:705] Step 8400 per-step time 0.075s\n","INFO:tensorflow:{'Loss/classification_loss': 0.13286045,\n"," 'Loss/localization_loss': 0.0681265,\n"," 'Loss/regularization_loss': 0.1290858,\n"," 'Loss/total_loss': 0.33007276,\n"," 'learning_rate': 0.07558186}\n","I1226 13:29:39.616314 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.13286045,\n"," 'Loss/localization_loss': 0.0681265,\n"," 'Loss/regularization_loss': 0.1290858,\n"," 'Loss/total_loss': 0.33007276,\n"," 'learning_rate': 0.07558186}\n","INFO:tensorflow:Step 8500 per-step time 0.074s\n","I1226 13:29:47.072360 140232156051328 model_lib_v2.py:705] Step 8500 per-step time 0.074s\n","INFO:tensorflow:{'Loss/classification_loss': 0.13807566,\n"," 'Loss/localization_loss': 0.10397038,\n"," 'Loss/regularization_loss': 0.12860847,\n"," 'Loss/total_loss': 0.37065452,\n"," 'learning_rate': 0.07546397}\n","I1226 13:29:47.072737 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.13807566,\n"," 'Loss/localization_loss': 0.10397038,\n"," 'Loss/regularization_loss': 0.12860847,\n"," 'Loss/total_loss': 0.37065452,\n"," 'learning_rate': 0.07546397}\n","INFO:tensorflow:Step 8600 per-step time 0.071s\n","I1226 13:29:54.150817 140232156051328 model_lib_v2.py:705] Step 8600 per-step time 0.071s\n","INFO:tensorflow:{'Loss/classification_loss': 0.268444,\n"," 'Loss/localization_loss': 0.10739611,\n"," 'Loss/regularization_loss': 0.12815408,\n"," 'Loss/total_loss': 0.5039942,\n"," 'learning_rate': 0.075344615}\n","I1226 13:29:54.151106 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.268444,\n"," 'Loss/localization_loss': 0.10739611,\n"," 'Loss/regularization_loss': 0.12815408,\n"," 'Loss/total_loss': 0.5039942,\n"," 'learning_rate': 0.075344615}\n","INFO:tensorflow:Step 8700 per-step time 0.074s\n","I1226 13:30:01.557534 140232156051328 model_lib_v2.py:705] Step 8700 per-step time 0.074s\n","INFO:tensorflow:{'Loss/classification_loss': 0.14302951,\n"," 'Loss/localization_loss': 0.084478095,\n"," 'Loss/regularization_loss': 0.12778467,\n"," 'Loss/total_loss': 0.35529226,\n"," 'learning_rate': 0.07522382}\n","I1226 13:30:01.557815 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.14302951,\n"," 'Loss/localization_loss': 0.084478095,\n"," 'Loss/regularization_loss': 0.12778467,\n"," 'Loss/total_loss': 0.35529226,\n"," 'learning_rate': 0.07522382}\n","INFO:tensorflow:Step 8800 per-step time 0.075s\n","I1226 13:30:09.069075 140232156051328 model_lib_v2.py:705] Step 8800 per-step time 0.075s\n","INFO:tensorflow:{'Loss/classification_loss': 0.09784015,\n"," 'Loss/localization_loss': 0.06895094,\n"," 'Loss/regularization_loss': 0.12730652,\n"," 'Loss/total_loss': 0.2940976,\n"," 'learning_rate': 0.07510157}\n","I1226 13:30:09.069385 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.09784015,\n"," 'Loss/localization_loss': 0.06895094,\n"," 'Loss/regularization_loss': 0.12730652,\n"," 'Loss/total_loss': 0.2940976,\n"," 'learning_rate': 0.07510157}\n","INFO:tensorflow:Step 8900 per-step time 0.075s\n","I1226 13:30:16.567831 140232156051328 model_lib_v2.py:705] Step 8900 per-step time 0.075s\n","INFO:tensorflow:{'Loss/classification_loss': 0.11389088,\n"," 'Loss/localization_loss': 0.14488703,\n"," 'Loss/regularization_loss': 0.12689695,\n"," 'Loss/total_loss': 0.38567486,\n"," 'learning_rate': 0.074977875}\n","I1226 13:30:16.568129 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.11389088,\n"," 'Loss/localization_loss': 0.14488703,\n"," 'Loss/regularization_loss': 0.12689695,\n"," 'Loss/total_loss': 0.38567486,\n"," 'learning_rate': 0.074977875}\n","INFO:tensorflow:Step 9000 per-step time 0.074s\n","I1226 13:30:23.962004 140232156051328 model_lib_v2.py:705] Step 9000 per-step time 0.074s\n","INFO:tensorflow:{'Loss/classification_loss': 0.10682689,\n"," 'Loss/localization_loss': 0.06795721,\n"," 'Loss/regularization_loss': 0.12643263,\n"," 'Loss/total_loss': 0.30121672,\n"," 'learning_rate': 0.07485275}\n","I1226 13:30:23.962338 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.10682689,\n"," 'Loss/localization_loss': 0.06795721,\n"," 'Loss/regularization_loss': 0.12643263,\n"," 'Loss/total_loss': 0.30121672,\n"," 'learning_rate': 0.07485275}\n","INFO:tensorflow:Step 9100 per-step time 0.085s\n","I1226 13:30:32.422847 140232156051328 model_lib_v2.py:705] Step 9100 per-step time 0.085s\n","INFO:tensorflow:{'Loss/classification_loss': 0.10677724,\n"," 'Loss/localization_loss': 0.042551618,\n"," 'Loss/regularization_loss': 0.12604111,\n"," 'Loss/total_loss': 0.27536997,\n"," 'learning_rate': 0.07472619}\n","I1226 13:30:32.423134 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.10677724,\n"," 'Loss/localization_loss': 0.042551618,\n"," 'Loss/regularization_loss': 0.12604111,\n"," 'Loss/total_loss': 0.27536997,\n"," 'learning_rate': 0.07472619}\n","INFO:tensorflow:Step 9200 per-step time 0.075s\n","I1226 13:30:39.887769 140232156051328 model_lib_v2.py:705] Step 9200 per-step time 0.075s\n","INFO:tensorflow:{'Loss/classification_loss': 0.11012757,\n"," 'Loss/localization_loss': 0.040499892,\n"," 'Loss/regularization_loss': 0.12559748,\n"," 'Loss/total_loss': 0.27622494,\n"," 'learning_rate': 0.07459819}\n","I1226 13:30:39.888129 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.11012757,\n"," 'Loss/localization_loss': 0.040499892,\n"," 'Loss/regularization_loss': 0.12559748,\n"," 'Loss/total_loss': 0.27622494,\n"," 'learning_rate': 0.07459819}\n","INFO:tensorflow:Step 9300 per-step time 0.075s\n","I1226 13:30:47.420431 140232156051328 model_lib_v2.py:705] Step 9300 per-step time 0.075s\n","INFO:tensorflow:{'Loss/classification_loss': 0.10374718,\n"," 'Loss/localization_loss': 0.033956766,\n"," 'Loss/regularization_loss': 0.12523708,\n"," 'Loss/total_loss': 0.26294103,\n"," 'learning_rate': 0.074468784}\n","I1226 13:30:47.420741 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.10374718,\n"," 'Loss/localization_loss': 0.033956766,\n"," 'Loss/regularization_loss': 0.12523708,\n"," 'Loss/total_loss': 0.26294103,\n"," 'learning_rate': 0.074468784}\n","INFO:tensorflow:Step 9400 per-step time 0.074s\n","I1226 13:30:54.822979 140232156051328 model_lib_v2.py:705] Step 9400 per-step time 0.074s\n","INFO:tensorflow:{'Loss/classification_loss': 0.105189145,\n"," 'Loss/localization_loss': 0.08100964,\n"," 'Loss/regularization_loss': 0.12492443,\n"," 'Loss/total_loss': 0.31112322,\n"," 'learning_rate': 0.074337944}\n","I1226 13:30:54.823355 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.105189145,\n"," 'Loss/localization_loss': 0.08100964,\n"," 'Loss/regularization_loss': 0.12492443,\n"," 'Loss/total_loss': 0.31112322,\n"," 'learning_rate': 0.074337944}\n","INFO:tensorflow:Step 9500 per-step time 0.073s\n","I1226 13:31:02.181767 140232156051328 model_lib_v2.py:705] Step 9500 per-step time 0.073s\n","INFO:tensorflow:{'Loss/classification_loss': 0.08455947,\n"," 'Loss/localization_loss': 0.07517225,\n"," 'Loss/regularization_loss': 0.12445196,\n"," 'Loss/total_loss': 0.28418368,\n"," 'learning_rate': 0.074205704}\n","I1226 13:31:02.182091 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.08455947,\n"," 'Loss/localization_loss': 0.07517225,\n"," 'Loss/regularization_loss': 0.12445196,\n"," 'Loss/total_loss': 0.28418368,\n"," 'learning_rate': 0.074205704}\n","INFO:tensorflow:Step 9600 per-step time 0.093s\n","I1226 13:31:11.496267 140232156051328 model_lib_v2.py:705] Step 9600 per-step time 0.093s\n","INFO:tensorflow:{'Loss/classification_loss': 0.13804294,\n"," 'Loss/localization_loss': 0.09675781,\n"," 'Loss/regularization_loss': 0.12399257,\n"," 'Loss/total_loss': 0.35879332,\n"," 'learning_rate': 0.07407206}\n","I1226 13:31:11.496615 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.13804294,\n"," 'Loss/localization_loss': 0.09675781,\n"," 'Loss/regularization_loss': 0.12399257,\n"," 'Loss/total_loss': 0.35879332,\n"," 'learning_rate': 0.07407206}\n","INFO:tensorflow:Step 9700 per-step time 0.103s\n","I1226 13:31:21.837714 140232156051328 model_lib_v2.py:705] Step 9700 per-step time 0.103s\n","INFO:tensorflow:{'Loss/classification_loss': 0.09856354,\n"," 'Loss/localization_loss': 0.04843226,\n"," 'Loss/regularization_loss': 0.123806715,\n"," 'Loss/total_loss': 0.2708025,\n"," 'learning_rate': 0.073937014}\n","I1226 13:31:21.838035 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.09856354,\n"," 'Loss/localization_loss': 0.04843226,\n"," 'Loss/regularization_loss': 0.123806715,\n"," 'Loss/total_loss': 0.2708025,\n"," 'learning_rate': 0.073937014}\n","INFO:tensorflow:Step 9800 per-step time 0.073s\n","I1226 13:31:29.090196 140232156051328 model_lib_v2.py:705] Step 9800 per-step time 0.073s\n","INFO:tensorflow:{'Loss/classification_loss': 0.16797222,\n"," 'Loss/localization_loss': 0.08036764,\n"," 'Loss/regularization_loss': 0.12335661,\n"," 'Loss/total_loss': 0.37169647,\n"," 'learning_rate': 0.07380057}\n","I1226 13:31:29.090512 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.16797222,\n"," 'Loss/localization_loss': 0.08036764,\n"," 'Loss/regularization_loss': 0.12335661,\n"," 'Loss/total_loss': 0.37169647,\n"," 'learning_rate': 0.07380057}\n","INFO:tensorflow:Step 9900 per-step time 0.075s\n","I1226 13:31:36.571900 140232156051328 model_lib_v2.py:705] Step 9900 per-step time 0.075s\n","INFO:tensorflow:{'Loss/classification_loss': 0.108741894,\n"," 'Loss/localization_loss': 0.102433965,\n"," 'Loss/regularization_loss': 0.12294724,\n"," 'Loss/total_loss': 0.3341231,\n"," 'learning_rate': 0.073662736}\n","I1226 13:31:36.572196 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.108741894,\n"," 'Loss/localization_loss': 0.102433965,\n"," 'Loss/regularization_loss': 0.12294724,\n"," 'Loss/total_loss': 0.3341231,\n"," 'learning_rate': 0.073662736}\n","INFO:tensorflow:Step 10000 per-step time 0.075s\n","I1226 13:31:44.068073 140232156051328 model_lib_v2.py:705] Step 10000 per-step time 0.075s\n","INFO:tensorflow:{'Loss/classification_loss': 0.060766403,\n"," 'Loss/localization_loss': 0.026396837,\n"," 'Loss/regularization_loss': 0.12258136,\n"," 'Loss/total_loss': 0.2097446,\n"," 'learning_rate': 0.07352352}\n","I1226 13:31:44.068424 140232156051328 model_lib_v2.py:708] {'Loss/classification_loss': 0.060766403,\n"," 'Loss/localization_loss': 0.026396837,\n"," 'Loss/regularization_loss': 0.12258136,\n"," 'Loss/total_loss': 0.2097446,\n"," 'learning_rate': 0.07352352}\n"]}],"source":["!{command}"]},{"cell_type":"markdown","metadata":{"id":"4_YRZu7npfDH"},"source":["# 7. Evaluate the Model"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"80L7-fdPpfDH","executionInfo":{"status":"ok","timestamp":1672061508019,"user_tz":-330,"elapsed":13,"user":{"displayName":"Mani","userId":"17004703946425084322"}}},"outputs":[],"source":["command = \"python {} --model_dir={} --pipeline_config_path={} --checkpoint_dir={}\".format(TRAINING_SCRIPT, paths['CHECKPOINT_PATH'],files['PIPELINE_CONFIG'], paths['CHECKPOINT_PATH'])"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"lYsgEPx9pfDH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672061508024,"user_tz":-330,"elapsed":15,"user":{"displayName":"Mani","userId":"17004703946425084322"}},"outputId":"60cf4068-82c1-4f39-f3ad-1e8c6992f9b7"},"outputs":[{"output_type":"stream","name":"stdout","text":["python Tensorflow/models/research/object_detection/model_main_tf2.py --model_dir=Tensorflow/workspace/models/my_ssd_mobnet --pipeline_config_path=Tensorflow/workspace/models/my_ssd_mobnet/pipeline.config --checkpoint_dir=Tensorflow/workspace/models/my_ssd_mobnet\n"]}],"source":["print(command)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lqTV2jGBpfDH"},"outputs":[],"source":["!{command}"]},{"cell_type":"markdown","metadata":{"id":"orvRk02UpfDI"},"source":["# 8. Load Train Model From Checkpoint"]},{"cell_type":"code","source":[],"metadata":{"id":"xH6nokquD9Fd"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":41,"metadata":{"id":"8TYk4_oIpfDI","executionInfo":{"status":"ok","timestamp":1672061650916,"user_tz":-330,"elapsed":1254,"user":{"displayName":"Mani","userId":"17004703946425084322"}}},"outputs":[],"source":["import os\n","import tensorflow as tf\n","from object_detection.utils import label_map_util\n","from object_detection.utils import visualization_utils as viz_utils\n","from object_detection.builders import model_builder\n","from object_detection.utils import config_util"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"uPip7dtl-l0X","executionInfo":{"status":"ok","timestamp":1672061657377,"user_tz":-330,"elapsed":482,"user":{"displayName":"Mani","userId":"17004703946425084322"}}},"outputs":[],"source":["# Prevent GPU complete consumption\n","gpus = tf.config.list_physical_devices('GPU')\n","if gpus:\n","    try: \n","        tf.config.experimental.set_virtual_device_configuration(\n","            gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)])\n","    except RunTimeError as e:\n","        print(e)"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"tDnQg-cYpfDI","executionInfo":{"status":"ok","timestamp":1672061659788,"user_tz":-330,"elapsed":730,"user":{"displayName":"Mani","userId":"17004703946425084322"}}},"outputs":[],"source":["# Load pipeline config and build a detection model\n","configs = config_util.get_configs_from_pipeline_file(files['PIPELINE_CONFIG'])\n","detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n","\n","# Restore checkpoint\n","ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n","ckpt.restore(os.path.join(paths['CHECKPOINT_PATH'], 'ckpt-9')).expect_partial()\n","\n","@tf.function\n","def detect_fn(image):\n","    image, shapes = detection_model.preprocess(image)\n","    prediction_dict = detection_model.predict(image, shapes)\n","    detections = detection_model.postprocess(prediction_dict, shapes)\n","    return detections"]},{"cell_type":"markdown","metadata":{"id":"0EmsmbBZpfDI"},"source":["# 9. Detect from an Image"]},{"cell_type":"code","execution_count":54,"metadata":{"id":"Y_MKiuZ4pfDI","executionInfo":{"status":"ok","timestamp":1672061980028,"user_tz":-330,"elapsed":851,"user":{"displayName":"Mani","userId":"17004703946425084322"}}},"outputs":[],"source":["import cv2 \n","import numpy as np\n","from matplotlib import pyplot as plt\n","%matplotlib inline"]},{"cell_type":"code","execution_count":55,"metadata":{"id":"cBDbIhNapfDI","executionInfo":{"status":"ok","timestamp":1672061980773,"user_tz":-330,"elapsed":7,"user":{"displayName":"Mani","userId":"17004703946425084322"}}},"outputs":[],"source":["category_index = label_map_util.create_category_index_from_labelmap(files['LABELMAP'])"]},{"cell_type":"code","execution_count":56,"metadata":{"id":"Lx3crOhOzITB","executionInfo":{"status":"ok","timestamp":1672061982173,"user_tz":-330,"elapsed":6,"user":{"displayName":"Mani","userId":"17004703946425084322"}}},"outputs":[],"source":["IMAGE_PATH = os.path.join(paths['IMAGE_PATH'], 'Cars426.png')"]},{"cell_type":"code","execution_count":57,"metadata":{"id":"Tpzn1SMry1yK","colab":{"base_uri":"https://localhost:8080/","height":450},"executionInfo":{"status":"error","timestamp":1672061984174,"user_tz":-330,"elapsed":7,"user":{"displayName":"Mani","userId":"17004703946425084322"}},"outputId":"d8dad01a-bce2-4720-85a4-23ceebb5dd48"},"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-57-999d48cacdb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnum_detections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'num_detections'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"]}],"source":["img = cv2.imread(IMAGE_PATH)\n","image_np = np.array(img)\n","\n","input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n","detections = detect_fn(input_tensor)\n","\n","num_detections = int(detections.pop('num_detections'))\n","detections = {key: value[0, :num_detections].numpy()\n","              for key, value in detections.items()}\n","detections['num_detections'] = num_detections\n","\n","# detection_classes should be ints.\n","detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n","\n","label_id_offset = 1\n","image_np_with_detections = image_np.copy()\n","\n","viz_utils.visualize_boxes_and_labels_on_image_array(\n","            image_np_with_detections,\n","            detections['detection_boxes'],\n","            detections['detection_classes']+label_id_offset,\n","            detections['detection_scores'],\n","            category_index,\n","            use_normalized_coordinates=True,\n","            max_boxes_to_draw=5,\n","            min_score_thresh=.8,\n","            agnostic_mode=False)\n","\n","plt.imshow(cv2.cvtColor(image_np_with_detections, cv2.COLOR_BGR2RGB))\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UacWg1Ln-l0e","executionInfo":{"status":"aborted","timestamp":1672061665167,"user_tz":-330,"elapsed":11,"user":{"displayName":"Mani","userId":"17004703946425084322"}}},"outputs":[],"source":["detections.keys()"]},{"cell_type":"markdown","metadata":{"id":"ADx_ZCfC-l0e"},"source":["# Apply OCR to Detection"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jyA7sf-5-l0f","colab":{"base_uri":"https://localhost:8080/","height":840},"executionInfo":{"status":"ok","timestamp":1671370623051,"user_tz":300,"elapsed":10312,"user":{"displayName":"ludena k","userId":"15289698329025880263"}},"outputId":"e2813243-e712-4ae1-a7e4-2aeb6cd0a907"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting easyocr\n","  Downloading easyocr-1.6.2-py3-none-any.whl (2.9 MB)\n","\u001b[K     |████████████████████████████████| 2.9 MB 5.5 MB/s \n","\u001b[?25hRequirement already satisfied: Shapely in /usr/local/lib/python3.8/dist-packages (from easyocr) (2.0.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from easyocr) (1.22.4)\n","Collecting opencv-python-headless<=4.5.4.60\n","  Downloading opencv_python_headless-4.5.4.60-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.6 MB)\n","\u001b[K     |████████████████████████████████| 47.6 MB 1.1 MB/s \n","\u001b[?25hRequirement already satisfied: scikit-image in /usr/local/lib/python3.8/dist-packages (from easyocr) (0.18.3)\n","Collecting python-bidi\n","  Downloading python_bidi-0.4.2-py2.py3-none-any.whl (30 kB)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from easyocr) (7.1.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from easyocr) (1.7.3)\n","Collecting ninja\n","  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n","\u001b[K     |████████████████████████████████| 145 kB 69.6 MB/s \n","\u001b[?25hRequirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.8/dist-packages (from easyocr) (0.9.1+cu111)\n","Collecting pyclipper\n","  Downloading pyclipper-1.3.0.post4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (619 kB)\n","\u001b[K     |████████████████████████████████| 619 kB 79.3 MB/s \n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from easyocr) (1.8.1+cu111)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from easyocr) (5.4.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->easyocr) (3.7.4.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from python-bidi->easyocr) (1.15.0)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image->easyocr) (1.4.1)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image->easyocr) (2.8.8)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image->easyocr) (2.9.0)\n","Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image->easyocr) (3.2.2)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image->easyocr) (2022.10.10)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->easyocr) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->easyocr) (1.4.4)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->easyocr) (2.4.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->easyocr) (0.11.0)\n","Installing collected packages: python-bidi, pyclipper, opencv-python-headless, ninja, easyocr\n","  Attempting uninstall: opencv-python-headless\n","    Found existing installation: opencv-python-headless 4.6.0.66\n","    Uninstalling opencv-python-headless-4.6.0.66:\n","      Successfully uninstalled opencv-python-headless-4.6.0.66\n","Successfully installed easyocr-1.6.2 ninja-1.11.1 opencv-python-headless-4.5.4.60 pyclipper-1.3.0.post4 python-bidi-0.4.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["cv2"]}}},"metadata":{}}],"source":["!pip install easyocr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WxGpoOKO-l0f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671370411985,"user_tz":300,"elapsed":237054,"user":{"displayName":"ludena k","userId":"15289698329025880263"}},"outputId":"36d958c5-615c-41da-88d9-6a7bcbf66282"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Collecting torch==1.8.1+cu111\n","  Downloading https://download.pytorch.org/whl/cu111/torch-1.8.1%2Bcu111-cp38-cp38-linux_x86_64.whl (1982.2 MB)\n","\u001b[K     |█████████████▌                  | 834.1 MB 1.7 MB/s eta 0:11:20tcmalloc: large alloc 1147494400 bytes == 0x39122000 @  0x7fcbf2f25615 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x4fd8b5 0x4997c7 0x4fd8b5 0x49abe4 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x5d8868 0x5da092 0x587116 0x5d8d8c 0x55dc1e 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x4990ca 0x5d8868 0x4997a2 0x4fd8b5 0x49abe4\n","\u001b[K     |█████████████████               | 1055.7 MB 1.2 MB/s eta 0:12:34tcmalloc: large alloc 1434370048 bytes == 0x7d778000 @  0x7fcbf2f25615 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x4fd8b5 0x4997c7 0x4fd8b5 0x49abe4 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x5d8868 0x5da092 0x587116 0x5d8d8c 0x55dc1e 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x4990ca 0x5d8868 0x4997a2 0x4fd8b5 0x49abe4\n","\u001b[K     |█████████████████████▋          | 1336.2 MB 1.2 MB/s eta 0:08:37tcmalloc: large alloc 1792966656 bytes == 0x25aa000 @  0x7fcbf2f25615 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x4fd8b5 0x4997c7 0x4fd8b5 0x49abe4 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x5d8868 0x5da092 0x587116 0x5d8d8c 0x55dc1e 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x4990ca 0x5d8868 0x4997a2 0x4fd8b5 0x49abe4\n","\u001b[K     |███████████████████████████▎    | 1691.1 MB 1.2 MB/s eta 0:04:13tcmalloc: large alloc 2241208320 bytes == 0x6d392000 @  0x7fcbf2f25615 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x4fd8b5 0x4997c7 0x4fd8b5 0x49abe4 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x5d8868 0x5da092 0x587116 0x5d8d8c 0x55dc1e 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x4990ca 0x5d8868 0x4997a2 0x4fd8b5 0x49abe4\n","\u001b[K     |████████████████████████████████| 1982.2 MB 1.2 MB/s eta 0:00:01tcmalloc: large alloc 1982177280 bytes == 0xf2cf4000 @  0x7fcbf2f241e7 0x4d30a0 0x4d312c 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x5d8868 0x4997a2 0x55cd91 0x5d8941 0x49abe4 0x4fd8b5 0x49abe4 0x55cd91\n","tcmalloc: large alloc 2477727744 bytes == 0x1dd3e0000 @  0x7fcbf2f25615 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x5d8868 0x4997a2 0x55cd91 0x5d8941 0x49abe4 0x4fd8b5 0x49abe4 0x55cd91 0x5d8941 0x4fe318\n","\u001b[K     |████████████████████████████████| 1982.2 MB 6.0 kB/s \n","\u001b[?25hCollecting torchvision==0.9.1+cu111\n","  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.9.1%2Bcu111-cp38-cp38-linux_x86_64.whl (17.6 MB)\n","\u001b[K     |████████████████████████████████| 17.6 MB 26.7 MB/s \n","\u001b[?25hCollecting torchaudio===0.8.1\n","  Downloading torchaudio-0.8.1-cp38-cp38-manylinux1_x86_64.whl (1.9 MB)\n","\u001b[K     |████████████████████████████████| 1.9 MB 7.4 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.8.1+cu111) (3.7.4.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torch==1.8.1+cu111) (1.22.4)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.9.1+cu111) (7.1.2)\n","Installing collected packages: torch, torchvision, torchaudio\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.13.0+cu116\n","    Uninstalling torch-1.13.0+cu116:\n","      Successfully uninstalled torch-1.13.0+cu116\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.14.0+cu116\n","    Uninstalling torchvision-0.14.0+cu116:\n","      Successfully uninstalled torchvision-0.14.0+cu116\n","  Attempting uninstall: torchaudio\n","    Found existing installation: torchaudio 0.13.0+cu116\n","    Uninstalling torchaudio-0.13.0+cu116:\n","      Successfully uninstalled torchaudio-0.13.0+cu116\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.14.0 requires torch==1.13.0, but you have torch 1.8.1+cu111 which is incompatible.\u001b[0m\n","Successfully installed torch-1.8.1+cu111 torchaudio-0.8.1 torchvision-0.9.1+cu111\n"]}],"source":["!pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f https://download.pytorch.org/whl/torch_stable.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MnuVry_G-l0g"},"outputs":[],"source":["import easyocr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ASF4hxcq-l0g"},"outputs":[],"source":["detection_threshold = 0.7"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O-lvLtrM-l0h"},"outputs":[],"source":["image = image_np_with_detections\n","scores = list(filter(lambda x: x> detection_threshold, detections['detection_scores']))\n","boxes = detections['detection_boxes'][:len(scores)]\n","classes = detections['detection_classes'][:len(scores)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tLXx-Ur1-l0i"},"outputs":[],"source":["width = image.shape[1]\n","height = image.shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NKxAmr7D-l0i","colab":{"base_uri":"https://localhost:8080/","height":187},"executionInfo":{"status":"ok","timestamp":1671375057140,"user_tz":300,"elapsed":3016,"user":{"displayName":"ludena k","userId":"15289698329025880263"}},"outputId":"40e9368e-8dff-4dd4-f1a1-171b39474588"},"outputs":[{"output_type":"stream","name":"stdout","text":["[0.6375012  0.39107394 0.7299045  0.6143098 ]\n","[148.53777498 156.42957687 170.06774217 245.7239151 ]\n","[([[9, 3], [83, 3], [83, 19], [9, 19]], 'IMAU 555', 0.895299785632388)]\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["# Apply ROI filtering and OCR\n","for idx, box in enumerate(boxes):\n","    print(box)\n","    roi = box*[height, width, height, width]\n","    print(roi)\n","    region = image[int(roi[0]):int(roi[2]),int(roi[1]):int(roi[3])]\n","    reader = easyocr.Reader(['en'])\n","    ocr_result = reader.readtext(region)\n","    print(ocr_result)\n","    plt.imshow(cv2.cvtColor(region, cv2.COLOR_BGR2RGB))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8f4XPoSD-l0j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671375062141,"user_tz":300,"elapsed":16,"user":{"displayName":"ludena k","userId":"15289698329025880263"}},"outputId":"be29725c-fc5b-43a9-d269-1b55b5cb0ce8"},"outputs":[{"output_type":"stream","name":"stdout","text":["16\n","IMAU 555\n"]}],"source":["for result in ocr_result:\n","    print(np.sum(np.subtract(result[0][2],result[0][1])))\n","    print(result[1])"]},{"cell_type":"markdown","metadata":{"id":"u3OWqtd6-l0k"},"source":["# OCR Filtering"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oKMK5AFZ-l0l"},"outputs":[],"source":["region_threshold = 0.05"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ltOYs2vd-l0l"},"outputs":[],"source":["def filter_text(region, ocr_result, region_threshold):\n","    rectangle_size = region.shape[0]*region.shape[1]\n","    \n","    plate = [] \n","    for result in ocr_result:\n","        length = np.sum(np.subtract(result[0][1], result[0][0]))\n","        height = np.sum(np.subtract(result[0][2], result[0][1]))\n","        \n","        if length*height / rectangle_size > region_threshold:\n","            plate.append(result[1])\n","    return plate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BBp1Ozrp-l0m","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671371536452,"user_tz":300,"elapsed":8,"user":{"displayName":"ludena k","userId":"15289698329025880263"}},"outputId":"114b2faa-27a1-464f-e119-6a4db07e62bf"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['IMAU 555']"]},"metadata":{},"execution_count":65}],"source":["filter_text(region, ocr_result, region_threshold)"]},{"cell_type":"markdown","metadata":{"id":"brN0KPrO-l0m"},"source":["# Bring it Together"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JaplC9A5-l0n"},"outputs":[],"source":["region_threshold = 0.6"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yGCpSknl-l0o"},"outputs":[],"source":["def ocr_it(image, detections, detection_threshold, region_threshold):\n","    \n","    # Scores, boxes and classes above threhold\n","    scores = list(filter(lambda x: x> detection_threshold, detections['detection_scores']))\n","    boxes = detections['detection_boxes'][:len(scores)]\n","    classes = detections['detection_classes'][:len(scores)]\n","    \n","    # Full image dimensions\n","    width = image.shape[1]\n","    height = image.shape[0]\n","    \n","    # Apply ROI filtering and OCR\n","    for idx, box in enumerate(boxes):\n","        roi = box*[height, width, height, width]\n","        region = image[int(roi[0]):int(roi[2]),int(roi[1]):int(roi[3])]\n","        reader = easyocr.Reader(['en'])\n","        ocr_result = reader.readtext(region)\n","        \n","        text = filter_text(region, ocr_result, region_threshold)\n","        \n","        plt.imshow(cv2.cvtColor(region, cv2.COLOR_BGR2RGB))\n","        plt.show()\n","        print(text)\n","        return text, region"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"2j-i8Tm6-l0p","colab":{"base_uri":"https://localhost:8080/","height":151},"executionInfo":{"status":"ok","timestamp":1671371675065,"user_tz":300,"elapsed":3147,"user":{"displayName":"ludena k","userId":"15289698329025880263"}},"outputId":"b0d1d21c-4f88-43c7-bf2e-62e4d6ac6352"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"stream","name":"stdout","text":["['IMAU 555']\n"]}],"source":["text, region = ocr_it(image_np_with_detections, detections, detection_threshold, region_threshold)"]},{"cell_type":"markdown","metadata":{"id":"HGxgeWhS-l0q"},"source":["# Save Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a31JDibe-l0r"},"outputs":[],"source":["import csv\n","import uuid"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IMuykJIQ-l0r","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1671371750938,"user_tz":300,"elapsed":15,"user":{"displayName":"ludena k","userId":"15289698329025880263"}},"outputId":"313c5f82-ce66-4ae1-b112-5ecf44187482"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'ae16e098-7edb-11ed-a2de-0242ac1c000c.jpg'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":70}],"source":["'{}.jpg'.format(uuid.uuid1())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"amx8vIej-l0s"},"outputs":[],"source":["def save_results(text, region, csv_filename, folder_path):\n","    img_name = '{}.jpg'.format(uuid.uuid1())\n","    \n","    cv2.imwrite(os.path.join(folder_path, img_name), region)\n","    \n","    with open(csv_filename, mode='a', newline='') as f:\n","        csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n","        csv_writer.writerow([img_name, text])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LlBz7m7P-l0t","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671371757997,"user_tz":300,"elapsed":7,"user":{"displayName":"ludena k","userId":"15289698329025880263"}},"outputId":"85aed694-d5c8-4664-85b4-d99d8cb33f2a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[ 76, 213, 248],\n","        [ 79, 210, 242],\n","        [ 90, 194, 218],\n","        ...,\n","        [162, 203, 212],\n","        [127, 187, 204],\n","        [ 72, 184, 218]],\n","\n","       [[ 72, 206, 245],\n","        [ 80, 196, 225],\n","        [139, 200, 213],\n","        ...,\n","        [ 74,  83,  83],\n","        [166, 178, 181],\n","        [ 96, 184, 209]],\n","\n","       [[ 68, 197, 237],\n","        [ 90, 195, 225],\n","        [162, 200, 209],\n","        ...,\n","        [  5,   5,   4],\n","        [136, 132, 130],\n","        [117, 190, 212]],\n","\n","       ...,\n","\n","       [[ 43, 105, 133],\n","        [ 89, 134, 156],\n","        [184, 200, 208],\n","        ...,\n","        [ 19,  23,  23],\n","        [161, 164, 165],\n","        [118, 150, 163]],\n","\n","       [[  2,  16,  24],\n","        [ 57,  70,  78],\n","        [207, 219, 221],\n","        ...,\n","        [ 54,  59,  60],\n","        [198, 209, 208],\n","        [109, 124, 127]],\n","\n","       [[  4,   6,   4],\n","        [ 12,  15,  15],\n","        [154, 161, 161],\n","        ...,\n","        [190, 200, 199],\n","        [200, 214, 212],\n","        [ 49,  56,  55]]], dtype=uint8)"]},"metadata":{},"execution_count":72}],"source":["region"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rhI5oi3P-l0u"},"outputs":[],"source":["save_results(text, region, 'detection_results.csv', 'Detection_Images')"]},{"cell_type":"markdown","source":["\n","#Webcam testing\n","\n","\n"],"metadata":{"id":"-XZK-kRhv31T"}},{"cell_type":"code","source":["# import dependencies\n","from IPython.display import display, Javascript, Image\n","from google.colab.output import eval_js\n","from base64 import b64decode, b64encode\n","import cv2\n","import numpy as np\n","import PIL\n","import io\n","import html\n","import time"],"metadata":{"id":"J4Ts6PobxuXV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def video_stream():\n","  js = Javascript('''\n","    var video;\n","    var div = null;\n","    var stream;\n","    var captureCanvas;\n","    var imgElement;\n","    var labelElement;\n","    \n","    var pendingResolve = null;\n","    var shutdown = false;\n","    \n","    function removeDom() {\n","       stream.getVideoTracks()[0].stop();\n","       video.remove();\n","       div.remove();\n","       video = null;\n","       div = null;\n","       stream = null;\n","       imgElement = null;\n","       captureCanvas = null;\n","       labelElement = null;\n","    }\n","    \n","    function onAnimationFrame() {\n","      if (!shutdown) {\n","        window.requestAnimationFrame(onAnimationFrame);\n","      }\n","      if (pendingResolve) {\n","        var result = \"\";\n","        if (!shutdown) {\n","          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n","          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n","        }\n","        var lp = pendingResolve;\n","        pendingResolve = null;\n","        lp(result);\n","      }\n","    }\n","    \n","    async function createDom() {\n","      if (div !== null) {\n","        return stream;\n","      }\n","\n","      div = document.createElement('div');\n","      div.style.border = '2px solid black';\n","      div.style.padding = '3px';\n","      div.style.width = '100%';\n","      div.style.maxWidth = '600px';\n","      document.body.appendChild(div);\n","      \n","      const modelOut = document.createElement('div');\n","      modelOut.innerHTML = \"<span>Status:</span>\";\n","      labelElement = document.createElement('span');\n","      labelElement.innerText = 'No data';\n","      labelElement.style.fontWeight = 'bold';\n","      modelOut.appendChild(labelElement);\n","      div.appendChild(modelOut);\n","           \n","      video = document.createElement('video');\n","      video.style.display = 'block';\n","      video.width = div.clientWidth - 6;\n","      video.setAttribute('playsinline', '');\n","      video.onclick = () => { shutdown = true; };\n","      stream = await navigator.mediaDevices.getUserMedia(\n","          {video: { facingMode: \"environment\"}});\n","      div.appendChild(video);\n","\n","      imgElement = document.createElement('img');\n","      imgElement.style.position = 'absolute';\n","      imgElement.style.zIndex = 1;\n","      imgElement.onclick = () => { shutdown = true; };\n","      div.appendChild(imgElement);\n","      \n","      const instruction = document.createElement('div');\n","      instruction.innerHTML = \n","          '<span style=\"color: red; font-weight: bold;\">' +\n","          'When finished, click here or on the video to stop this demo</span>';\n","      div.appendChild(instruction);\n","      instruction.onclick = () => { shutdown = true; };\n","      \n","      video.srcObject = stream;\n","      await video.play();\n","\n","      captureCanvas = document.createElement('canvas');\n","      captureCanvas.width = 640; //video.videoWidth;\n","      captureCanvas.height = 480; //video.videoHeight;\n","      window.requestAnimationFrame(onAnimationFrame);\n","      \n","      return stream;\n","    }\n","    async function stream_frame(label, imgData) {\n","      if (shutdown) {\n","        removeDom();\n","        shutdown = false;\n","        return '';\n","      }\n","\n","      var preCreate = Date.now();\n","      stream = await createDom();\n","      \n","      var preShow = Date.now();\n","      if (label != \"\") {\n","        labelElement.innerHTML = label;\n","      }\n","            \n","      if (imgData != \"\") {\n","        var videoRect = video.getClientRects()[0];\n","        imgElement.style.top = videoRect.top + \"px\";\n","        imgElement.style.left = videoRect.left + \"px\";\n","        imgElement.style.width = videoRect.width + \"px\";\n","        imgElement.style.height = videoRect.height + \"px\";\n","        imgElement.src = imgData;\n","      }\n","      \n","      var preCapture = Date.now();\n","      var result = await new Promise(function(resolve, reject) {\n","        pendingResolve = resolve;\n","      });\n","      shutdown = false;\n","      \n","      return {'create': preShow - preCreate, \n","              'show': preCapture - preShow, \n","              'capture': Date.now() - preCapture,\n","              'img': result};\n","    }\n","    ''')\n","\n","  display(js)\n","  \n","def video_frame(label, bbox):\n","  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n","  return data\n"],"metadata":{"id":"mclbLqOt1BeT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def js_to_image(js_reply):\n","  \"\"\"\n","  Params:\n","          js_reply: JavaScript object containing image from webcam\n","  Returns:\n","          img: OpenCV BGR image\n","  \"\"\"\n","  # decode base64 image\n","  image_bytes = b64decode(js_reply.split(',')[1])\n","  # convert bytes to numpy array\n","  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n","  # decode numpy array into OpenCV BGR image\n","  img = cv2.imdecode(jpg_as_np, flags=1)\n","\n","  return img\n","def bbox_to_bytes(bbox_array):\n","  \"\"\"\n","  Params:\n","          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n","  Returns:\n","        bytes: Base64 image byte string\n","  \"\"\"\n","  # convert array into PIL image\n","  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n","  iobuf = io.BytesIO()\n","  # format bbox into png for return\n","  bbox_PIL.save(iobuf, format='png')\n","  # format return string\n","  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n","\n","  return bbox_bytes\n"],"metadata":{"id":"2c0_wjHK2u98"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab.patches import cv2_imshow\n","# start streaming video from webcam\n","video_stream()\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","# label for video\n","label_html = 'Capturing...'\n","# initialze bounding box to empty\n","bbox = ''\n","count = 0 \n","while True:\n","    js_reply = video_frame(label_html, bbox)\n","    if not js_reply:\n","        break\n","    # convert JS response to OpenCV Image\n","    img = js_to_image(js_reply[\"img\"])\n","    # dection\n","    input_tensor = tf.convert_to_tensor(np.expand_dims(img, 0), dtype=tf.float32)\n","    detections = detect_fn(input_tensor)\n","    \n","    num_detections = int(detections.pop('num_detections'))\n","    detections = {key: value[0, :num_detections].numpy()\n","                  for key, value in detections.items()}\n","    detections['num_detections'] = num_detections\n","\n","    # detection_classes should be ints.\n","    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n","    bbox_array = img.copy()\n","    dect=detections['detection_boxes'][0]\n","    print(type(dect))\n","    for (x,y,w,h) in dect:\n","      bbox_array = cv2.rectangle(bbox_array,(x,y),(x+w,y+h),(255,0,0),2)\n","\n","    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n","    # convert overlay of bbox into bytes\n","    bbox_bytes = bbox_to_bytes(bbox_array)\n","    # update bbox so next frame gets new overlay\n","    bbox = bbox_bytes\n","\n","\n","    # print(detections['detection_boxes'],'  ',detections['detection_classes']+label_id_offset ,' -> ' ,detections['detection_scores'] ,'  ',category_index)\n","\n","    # label_id_offset = 1\n","    # print(type(detections['detection_boxes']))\n","    # image_np_with_detections = cv2.rectangle(image_np_with_detections, detections['detection_boxes'],(detections['detection_classes']+detections['detection_scores']),detections['detection_scores'],category_index)\n","    # print(detections['detection_boxes'],(detections['detection_classes']+detections['detection_scores']),detections['detection_scores'],category_index)\n","    # bbox=image_np_with_detections\n","    # print(detections['detection_scores'])\n","  \n","   \n","   \n","   \n","    # viz_utils.visualize_boxes_and_labels_on_image_array(\n","    #             image_np_with_detections,\n","    #             detections['detection_boxes'],\n","    #             detections['detection_classes']+label_id_offset,\n","    #             detections['detection_scores'],\n","    #             category_index,\n","    #             use_normalized_coordinates=True,\n","    #             max_boxes_to_draw=5,\n","    #             min_score_thresh=.8,\n","    #             agnostic_mode=0.1)\n","    # plt.figure(figsize=(800,600), dpi=200)\n","    # plt.axis(\"off\")\n","    # plt.imshow(image_np)\n","    # %matplotlib inline\n","    # plt.figure()\n","    # plt.imshow(image_np_with_detections)\n","    # print('Done')\n","    # plt.show()\n","\n","    # cv2_imshow('object detection',  cv2.resize(image_np_with_detections, (800, 600)))\n","    \n","    # if cv2.waitKey(10) & 0xFF == ord('q'):\n","    #     cap.release()\n","    #     cv2.destroyAllWindows()\n","    #     break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":753},"id":"f79_L_i17ah-","executionInfo":{"status":"error","timestamp":1671391111426,"user_tz":300,"elapsed":2941,"user":{"displayName":"ludena k","userId":"15289698329025880263"}},"outputId":"efd6a370-9785-49db-9e16-ab948ecce859"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    var video;\n","    var div = null;\n","    var stream;\n","    var captureCanvas;\n","    var imgElement;\n","    var labelElement;\n","    \n","    var pendingResolve = null;\n","    var shutdown = false;\n","    \n","    function removeDom() {\n","       stream.getVideoTracks()[0].stop();\n","       video.remove();\n","       div.remove();\n","       video = null;\n","       div = null;\n","       stream = null;\n","       imgElement = null;\n","       captureCanvas = null;\n","       labelElement = null;\n","    }\n","    \n","    function onAnimationFrame() {\n","      if (!shutdown) {\n","        window.requestAnimationFrame(onAnimationFrame);\n","      }\n","      if (pendingResolve) {\n","        var result = \"\";\n","        if (!shutdown) {\n","          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n","          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n","        }\n","        var lp = pendingResolve;\n","        pendingResolve = null;\n","        lp(result);\n","      }\n","    }\n","    \n","    async function createDom() {\n","      if (div !== null) {\n","        return stream;\n","      }\n","\n","      div = document.createElement('div');\n","      div.style.border = '2px solid black';\n","      div.style.padding = '3px';\n","      div.style.width = '100%';\n","      div.style.maxWidth = '600px';\n","      document.body.appendChild(div);\n","      \n","      const modelOut = document.createElement('div');\n","      modelOut.innerHTML = \"<span>Status:</span>\";\n","      labelElement = document.createElement('span');\n","      labelElement.innerText = 'No data';\n","      labelElement.style.fontWeight = 'bold';\n","      modelOut.appendChild(labelElement);\n","      div.appendChild(modelOut);\n","           \n","      video = document.createElement('video');\n","      video.style.display = 'block';\n","      video.width = div.clientWidth - 6;\n","      video.setAttribute('playsinline', '');\n","      video.onclick = () => { shutdown = true; };\n","      stream = await navigator.mediaDevices.getUserMedia(\n","          {video: { facingMode: \"environment\"}});\n","      div.appendChild(video);\n","\n","      imgElement = document.createElement('img');\n","      imgElement.style.position = 'absolute';\n","      imgElement.style.zIndex = 1;\n","      imgElement.onclick = () => { shutdown = true; };\n","      div.appendChild(imgElement);\n","      \n","      const instruction = document.createElement('div');\n","      instruction.innerHTML = \n","          '<span style=\"color: red; font-weight: bold;\">' +\n","          'When finished, click here or on the video to stop this demo</span>';\n","      div.appendChild(instruction);\n","      instruction.onclick = () => { shutdown = true; };\n","      \n","      video.srcObject = stream;\n","      await video.play();\n","\n","      captureCanvas = document.createElement('canvas');\n","      captureCanvas.width = 640; //video.videoWidth;\n","      captureCanvas.height = 480; //video.videoHeight;\n","      window.requestAnimationFrame(onAnimationFrame);\n","      \n","      return stream;\n","    }\n","    async function stream_frame(label, imgData) {\n","      if (shutdown) {\n","        removeDom();\n","        shutdown = false;\n","        return '';\n","      }\n","\n","      var preCreate = Date.now();\n","      stream = await createDom();\n","      \n","      var preShow = Date.now();\n","      if (label != \"\") {\n","        labelElement.innerHTML = label;\n","      }\n","            \n","      if (imgData != \"\") {\n","        var videoRect = video.getClientRects()[0];\n","        imgElement.style.top = videoRect.top + \"px\";\n","        imgElement.style.left = videoRect.left + \"px\";\n","        imgElement.style.width = videoRect.width + \"px\";\n","        imgElement.style.height = videoRect.height + \"px\";\n","        imgElement.src = imgData;\n","      }\n","      \n","      var preCapture = Date.now();\n","      var result = await new Promise(function(resolve, reject) {\n","        pendingResolve = resolve;\n","      });\n","      shutdown = false;\n","      \n","      return {'create': preShow - preCreate, \n","              'show': preCapture - preShow, \n","              'capture': Date.now() - preCapture,\n","              'img': result};\n","    }\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["<class 'numpy.ndarray'>\n"]},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-183-2921333445f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mdect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'detection_boxes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdect\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m       \u001b[0mbbox_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable numpy.float32 object"]}]},{"cell_type":"code","source":["from IPython.display import display, Javascript\n","from google.colab.output import eval_js\n","from base64 import b64decode\n","\n","def take_photo(filename='photo.jpg', quality=0.8):\n","  js = Javascript('''\n","    async function takePhoto(quality) {\n","      const div = document.createElement('div');\n","      const capture = document.createElement('button');\n","      capture.textContent = 'Capture';\n","      div.appendChild(capture);\n","\n","      const video = document.createElement('video');\n","      video.style.display = 'block';\n","      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n","\n","      document.body.appendChild(div);\n","      div.appendChild(video);\n","      video.srcObject = stream;\n","      await video.play();\n","\n","      // Resize the output to fit the video element.\n","      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n","\n","      // Wait for Capture to be clicked.\n","      await new Promise((resolve) => capture.onclick = resolve);\n","\n","      const canvas = document.createElement('canvas');\n","      canvas.width = video.videoWidth;\n","      canvas.height = video.videoHeight;\n","      canvas.getContext('2d').drawImage(video, 0, 0);\n","      stream.getVideoTracks()[0].stop();\n","      div.remove();\n","      return canvas.toDataURL('image/jpeg', quality);\n","    }\n","    ''')\n","  display(js)\n","  data = eval_js('takePhoto({})'.format(quality))\n","  binary = b64decode(data.split(',')[1])\n","  with open(filename, 'wb') as f:\n","    f.write(binary)\n","  return filename"],"metadata":{"id":"jlnxAi_tMOJI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import Image\n","try:\n","  filename = take_photo()\n","  print('Saved to {}'.format(filename))\n","  \n","  # Show the image which was just taken.\n","  display(Image(filename))\n","except Exception as err:\n","  # Errors will be thrown if the user does not have a webcam or if they do not\n","  # grant the page permission to access it.\n","  print(str(err))"],"metadata":{"id":"IAtfnkDkMOJN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IsNAaYAo0WVL"},"source":["# 10. Real Time Detections from your Webcam"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UKYVAzX1-l0v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671383895671,"user_tz":300,"elapsed":1346,"user":{"displayName":"ludena k","userId":"15289698329025880263"}},"outputId":"62881f1a-577f-4dce-a23d-be2a8f1afe74"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Skipping opencv-python-headless as it is not installed.\u001b[0m\n"]}],"source":["!pip uninstall opencv-python-headless -y"]},{"cell_type":"code","source":["cap = cv2.VideoCapture(1)\n","width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","\n","while cap.isOpened(): \n","    ret, frame = cap.read()\n","    image_np = np.array(frame)\n","    \n","    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n","    detections = detect_fn(input_tensor)\n","    \n","    num_detections = int(detections.pop('num_detections'))\n","    detections = {key: value[0, :num_detections].numpy()\n","                  for key, value in detections.items()}\n","    detections['num_detections'] = num_detections\n","\n","    # detection_classes should be ints.\n","    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n","\n","    label_id_offset = 1\n","    image_np_with_detections = image_np.copy()\n","    image_np_with_detections = cv2.rectangle(image_np_with_detections,(x,y),(x+w,y+h),(255,0,0),2)\n","    viz_utils.visualize_boxes_and_labels_on_image_array(\n","                image_np_with_detections,\n","                detections['detection_boxes'],\n","                detections['detection_classes']+label_id_offset,\n","                detections['detection_scores'],\n","                category_index,\n","                use_normalized_coordinates=True,\n","                max_boxes_to_draw=5,\n","                min_score_thresh=.8,\n","                agnostic_mode=False)\n","\n","    cv2.imshow('object detection',  cv2.resize(image_np_with_detections, (800, 600)))\n","    \n","    if cv2.waitKey(10) & 0xFF == ord('q'):\n","        cap.release()\n","        cv2.destroyAllWindows()\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":765},"id":"wPGwfaJwid7B","executionInfo":{"status":"error","timestamp":1671380564088,"user_tz":300,"elapsed":726,"user":{"displayName":"ludena k","userId":"15289698329025880263"}},"outputId":"3b159c13-f54d-4b8f-d95e-e60f6fc664aa"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-123-afcc57ccd35c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mnum_detections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'num_detections'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/__autograph_generated_filewsken6rl.py\u001b[0m in \u001b[0;36mtf__detect_fn\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefinedReturnValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetection_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 \u001b[0mprediction_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetection_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetection_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/object_detection/meta_architectures/ssd_meta_arch.py\u001b[0m in \u001b[0;36mtf__preprocess\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     13\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                         \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                         \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape_utils\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_images_and_return_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_image_resizer_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                         \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/object_detection/utils/shape_utils.py\u001b[0m in \u001b[0;36mtf__resize_images_and_return_shapes\u001b[0;34m(inputs, image_resizer_fn)\u001b[0m\n\u001b[1;32m     23\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melse_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatic_or_dynamic_map_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_resizer_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melems\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                 \u001b[0mresized_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mtrue_image_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/object_detection/utils/shape_utils.py\u001b[0m in \u001b[0;36mtf__static_or_dynamic_map_fn\u001b[0;34m(fn, elems, dtype, parallel_iterations, back_prop)\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0melem_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'elem_shapes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'outputs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m                 \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_body_5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melse_body_5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state_7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state_7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'do_return'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'outputs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'retval_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0mget_state_12\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/object_detection/utils/shape_utils.py\u001b[0m in \u001b[0;36melse_body_5\u001b[0;34m()\u001b[0m\n\u001b[1;32m    140\u001b[0m                         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'outputs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m                     \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mor_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melems_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melems_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_body_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melse_body_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state_6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state_6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'do_return'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'outputs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'retval_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m                 \u001b[0melem_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'elem_shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0melem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'elem'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/object_detection/utils/shape_utils.py\u001b[0m in \u001b[0;36melse_body_4\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m                     \u001b[0;32mdef\u001b[0m \u001b[0melse_body_4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                         \u001b[0;32mnonlocal\u001b[0m \u001b[0mretval_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'outputs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mor_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melems_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melems_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_body_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melse_body_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state_6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state_6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'do_return'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'outputs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'retval_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/object_detection/utils/shape_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    138\u001b[0m                     \u001b[0;32mdef\u001b[0m \u001b[0melse_body_4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                         \u001b[0;32mnonlocal\u001b[0m \u001b[0mretval_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'outputs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mor_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melems_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melems_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_body_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melse_body_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state_6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state_6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'do_return'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'outputs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'retval_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/object_detection/core/preprocessor.py\u001b[0m in \u001b[0;36mtf__resize_image\u001b[0;34m(image, masks, new_height, new_width, method, align_corners)\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefinedReturnValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ResizeImage'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malign_corners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                     \u001b[0mnew_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malign_corners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                     \u001b[0mimage_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape_utils\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombined_static_and_dynamic_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"<ipython-input-90-d301b2584748>\", line 11, in detect_fn  *\n        image, shapes = detection_model.preprocess(image)\n    File \"/usr/local/lib/python3.8/dist-packages/object_detection/meta_architectures/ssd_meta_arch.py\", line 484, in preprocess  *\n        normalized_inputs, self._image_resizer_fn)\n    File \"/usr/local/lib/python3.8/dist-packages/object_detection/utils/shape_utils.py\", line 492, in resize_images_and_return_shapes  *\n        outputs = static_or_dynamic_map_fn(\n    File \"/usr/local/lib/python3.8/dist-packages/object_detection/utils/shape_utils.py\", line 246, in static_or_dynamic_map_fn  *\n        outputs = [fn(arg) for arg in tf.unstack(elems)]\n    File \"/usr/local/lib/python3.8/dist-packages/object_detection/core/preprocessor.py\", line 3330, in resize_image  *\n        new_image = tf.image.resize_images(\n\n    ValueError: 'images' must have either 3 or 4 dimensions.\n"]}]},{"cell_type":"markdown","metadata":{"id":"rzlM4jt0pfDJ"},"source":["# 10. Freezing the Graph"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n4olHB2npfDJ"},"outputs":[],"source":["FREEZE_SCRIPT = os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection', 'exporter_main_v2.py ')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0AjO93QDpfDJ"},"outputs":[],"source":["command = \"python {} --input_type=image_tensor --pipeline_config_path={} --trained_checkpoint_dir={} --output_directory={}\".format(FREEZE_SCRIPT ,files['PIPELINE_CONFIG'], paths['CHECKPOINT_PATH'], paths['OUTPUT_PATH'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F6Lsp3tCpfDJ"},"outputs":[],"source":["print(command)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Sw1ULgHpfDJ"},"outputs":[],"source":["!{command}"]},{"cell_type":"markdown","metadata":{"id":"wTPmdqaXpfDK"},"source":["# 11. Conversion to TFJS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gZ6UzY_fpfDK","scrolled":true},"outputs":[],"source":["!pip install tensorflowjs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0oxbVynHpfDK"},"outputs":[],"source":["command = \"tensorflowjs_converter --input_format=tf_saved_model --output_node_names='detection_boxes,detection_classes,detection_features,detection_multiclass_scores,detection_scores,num_detections,raw_detection_boxes,raw_detection_scores' --output_format=tfjs_graph_model --signature_name=serving_default {} {}\".format(os.path.join(paths['OUTPUT_PATH'], 'saved_model'), paths['TFJS_PATH'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DB2AGNmJpfDK"},"outputs":[],"source":["print(command)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K7rfT4-hpfDK"},"outputs":[],"source":["!{command}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o8_hm-itpfDK"},"outputs":[],"source":["# Test Code: https://github.com/nicknochnack/RealTimeSignLanguageDetectionwithTFJS"]},{"cell_type":"markdown","metadata":{"id":"VtUw73FHpfDK"},"source":["# 12. Conversion to TFLite"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XviMtewLpfDK"},"outputs":[],"source":["TFLITE_SCRIPT = os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection', 'export_tflite_graph_tf2.py ')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"us86cjC4pfDL"},"outputs":[],"source":["command = \"python {} --pipeline_config_path={} --trained_checkpoint_dir={} --output_directory={}\".format(TFLITE_SCRIPT ,files['PIPELINE_CONFIG'], paths['CHECKPOINT_PATH'], paths['TFLITE_PATH'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n1r5YO3rpfDL"},"outputs":[],"source":["print(command)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I-xWpHN8pfDL"},"outputs":[],"source":["!{command}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iJfYMbN6pfDL"},"outputs":[],"source":["FROZEN_TFLITE_PATH = os.path.join(paths['TFLITE_PATH'], 'saved_model')\n","TFLITE_MODEL = os.path.join(paths['TFLITE_PATH'], 'saved_model', 'detect.tflite')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2I5CyxhG-l07"},"outputs":[],"source":["command = \"tflite_convert \\\n","--saved_model_dir={} \\\n","--output_file={} \\\n","--input_shapes=1,300,300,3 \\\n","--input_arrays=normalized_input_image_tensor \\\n","--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\\n","--inference_type=FLOAT \\\n","--allow_custom_ops\".format(FROZEN_TFLITE_PATH, TFLITE_MODEL, )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E8GwUeoFpfDL"},"outputs":[],"source":["print(command)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nbd7gqHMpfDL"},"outputs":[],"source":["!{command}"]},{"cell_type":"markdown","metadata":{"id":"5NQqZRdA21Uc"},"source":["# 13. Zip and Export Models "]},{"cell_type":"code","execution_count":18,"metadata":{"id":"tTVTGCQp2ZJJ","executionInfo":{"status":"ok","timestamp":1672059728266,"user_tz":-330,"elapsed":617,"user":{"displayName":"Mani","userId":"17004703946425084322"}}},"outputs":[],"source":["!tar -czf models.tar.gz {paths['CHECKPOINT_PATH']}"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"whShhB0x3PYJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672059750791,"user_tz":-330,"elapsed":21711,"user":{"displayName":"Mani","userId":"17004703946425084322"}},"outputId":"fee5aadb-c6af-4a72-ea33-924fb2fecd8d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["Github\n"],"metadata":{"id":"laTaief5qWZk"}}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/nicknochnack/RealTimeAutomaticNumberPlateRecognition/blob/main/Automatic%20Number%20Plate%20Detection.ipynb","timestamp":1671362687883}]},"kernelspec":{"display_name":"anprsys","language":"python","name":"anprsys"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}
